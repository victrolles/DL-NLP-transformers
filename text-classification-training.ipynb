{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Train Adapt Optimize (TAO) Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning is a technique of extracting learned features from an existing neural network to a new one. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task, when creating a large training dataset is not feasible. \n",
    "\n",
    "**Train Adapt Optimize (TAO) Toolkit** is a simple and easy-to-use python based AI toolkit for taking purpose-built pre-trained AI models and customizing them with users' own data.\n",
    "\n",
    "\n",
    "![Train Adapt Optimize (TAO) Toolkit](https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png)\n",
    "\n",
    "\n",
    "Let's see this in action with a use case for text classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "- Take a [BERT](https://arxiv.org/pdf/1810.04805.pdf) Text Classification model, [**Train**](#training) and [**Finetune**](#ft) it on the [SST-2 dataset](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip)\n",
    "- Run [**Evaluation**](#evaluation) and [**Inference**](#inference)\n",
    "- [**Export**](#export-onnx) the model for the [ONNX](https://onnx.ai/) format, or [export](#export-riva) to deployment on [Riva](https://developer.nvidia.com/riva).\n",
    "\n",
    "The earlier sections in the notebook give a brief introduction to the Text Classification task and the SST-2 dataset. If you are already familiar with these, and want to jump right into the meat of the matter, you can start at section on [Download and preprocess the dataset](#prepare-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "Please make sure of the following software requirements:\n",
    "\n",
    "1. python 3.6.9\n",
    "2. docker-ce > 19.03.5\n",
    "3. docker-API 1.40\n",
    "4. nvidia-container-toolkit > 1.3.0-1\n",
    "5. nvidia-container-runtime > 3.4.0-1\n",
    "6. nvidia-docker2 > 2.5.0-1\n",
    "7. nvidia-driver >= 455.23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the docker image versions and the tasks that tao can perform. You can also check this out with a `tao --help` or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/.tao_mounts.json wasn't found. Falling back to obtain mount points and docker configs from ~/.tlt_mounts.json.\n",
      "Please note that this will be deprecated going forward.\n",
      "Configuration of the TAO Toolkit Instance\n",
      "\n",
      "dockers: \t\t\n",
      "\tnvidia/tao/tao-toolkit-tf: \t\t\t\n",
      "\t\tv3.21.11-tf1.15.5-py3: \t\t\t\t\n",
      "\t\t\tdocker_registry: nvcr.io\n",
      "\t\t\ttasks: \n",
      "\t\t\t\t1. augment\n",
      "\t\t\t\t2. bpnet\n",
      "\t\t\t\t3. classification\n",
      "\t\t\t\t4. dssd\n",
      "\t\t\t\t5. emotionnet\n",
      "\t\t\t\t6. efficientdet\n",
      "\t\t\t\t7. fpenet\n",
      "\t\t\t\t8. gazenet\n",
      "\t\t\t\t9. gesturenet\n",
      "\t\t\t\t10. heartratenet\n",
      "\t\t\t\t11. lprnet\n",
      "\t\t\t\t12. mask_rcnn\n",
      "\t\t\t\t13. multitask_classification\n",
      "\t\t\t\t14. retinanet\n",
      "\t\t\t\t15. ssd\n",
      "\t\t\t\t16. unet\n",
      "\t\t\t\t17. yolo_v3\n",
      "\t\t\t\t18. yolo_v4\n",
      "\t\t\t\t19. yolo_v4_tiny\n",
      "\t\t\t\t20. converter\n",
      "\t\tv3.21.11-tf1.15.4-py3: \t\t\t\t\n",
      "\t\t\tdocker_registry: nvcr.io\n",
      "\t\t\ttasks: \n",
      "\t\t\t\t1. detectnet_v2\n",
      "\t\t\t\t2. faster_rcnn\n",
      "\tnvidia/tao/tao-toolkit-pyt: \t\t\t\n",
      "\t\tv3.21.11-py3: \t\t\t\t\n",
      "\t\t\tdocker_registry: nvcr.io\n",
      "\t\t\ttasks: \n",
      "\t\t\t\t1. speech_to_text\n",
      "\t\t\t\t2. speech_to_text_citrinet\n",
      "\t\t\t\t3. text_classification\n",
      "\t\t\t\t4. question_answering\n",
      "\t\t\t\t5. token_classification\n",
      "\t\t\t\t6. intent_slot_classification\n",
      "\t\t\t\t7. punctuation_and_capitalization\n",
      "\t\t\t\t8. spectro_gen\n",
      "\t\t\t\t9. vocoder\n",
      "\t\t\t\t10. action_recognition\n",
      "\tnvidia/tao/tao-toolkit-lm: \t\t\t\n",
      "\t\tv3.21.08-py3: \t\t\t\t\n",
      "\t\t\tdocker_registry: nvcr.io\n",
      "\t\t\ttasks: \n",
      "\t\t\t\t1. n_gram\n",
      "format_version: 2.0\n",
      "toolkit_version: 3.21.11\n",
      "published_date: 11/08/2021\n"
     ]
    }
   ],
   "source": [
    "!tao info --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the GPUs are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar  7 08:21:25 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   24C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "### Task Description\n",
    "Text Classification is one of the most common tasks in NLP, which is the process of categorizing the text into a group of words. By using NLP, text classification can automatically analyze text and then assign a set of predefined tags or categories based on its context. It is applied in a wide variety of applications, including sentiment analysis, spam filtering, news categorization, domain/intent detection for dialogue systems, etc. The dataset we use in this notebook, [SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip), pose this as a sentiment analysis task, i.e. given a piece of text, the goal is to estimate its sentiment polarity based solely on its content. \n",
    "\n",
    "For every entry in the training dataset, we predict the sentiment label of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "This model supports text classification problems such as sentiment analysis or domain/intent detection for dialogue systems, as long as the data follows the format specified below.\n",
    "\n",
    "The Text Classification Model requires the data to be stored in TAB separated files (.tsv) with two columns of sentence and label. Each line of the data file contains text sequences, where words are separated with spaces and label separated with [TAB], i.e.:\n",
    "\n",
    "    [WORD] [SPACE] [WORD] [SPACE] [WORD] [TAB] [LABEL]\n",
    "\n",
    "For example:\n",
    "\n",
    "    hide new secretions from the parental units [TAB] 0\n",
    "\n",
    "    that loves its characters and communicates something rather beautiful about human nature [TAB] 1\n",
    "\n",
    "    ...\n",
    "If your dataset is stored in another format, you need to convert it to this format to use the Text Classification Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The SST-2 Dataset\n",
    "\n",
    "The Stanford Sentiment Treebank dataset contains 215,154 phrases with fine-grained sentiment labels in the parse trees of 11,855 sentences from movie reviews. Model performances are evaluated either based on a fine-grained (5-way) or binary classification model based on accuracy. The [SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip) Binary classification version is identical to SST-1, but with neutral reviews deleted.\n",
    "\n",
    "The SST-2 format consists of a .tsv file for each dataset split, i.e. train, dev, and test data. Each entry has a space-separated sentence, followed by a tab and a label.\n",
    "\n",
    "For example:\n",
    "\n",
    "    sentence\tlabel\n",
    "    \n",
    "    excruciatingly unfunny and pitifully unromantic \t0\n",
    "    \n",
    "    enriched by an imaginatively mixed cast of antic spirits \t1\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prepare-data'></a>\n",
    "### Download and preprocess the dataset \n",
    "\n",
    "We provide a function, download_sst2, for downloading the data. Please refer to the TAO workflow section for the [data preprocessing and conversion part](#dataset-convert)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the dataset\n",
    "\n",
    "For convenience, you may use the code below to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# simple utility function to download the SST-2 dataset\n",
    "def download_sst2(save_path):\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    # url and filename of the dataset\n",
    "    download_urls = {\n",
    "            'https://dl.fbaipublicfiles.com/glue/data/SST-2.zip': 'SST-2.zip',\n",
    "    }\n",
    "         \n",
    "    for item in download_urls:\n",
    "        url = item\n",
    "        file = download_urls[item]\n",
    "        print('Downloading:', url)\n",
    "        if os.path.isfile(save_path + '/' + file):\n",
    "            print('** Download file already exists, skipping download **')\n",
    "        else:\n",
    "            response = urllib.request.urlopen(url)\n",
    "            with open(save_path + '/' + file, \"wb\") as handle:\n",
    "                handle.write(response.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT Set the following path\n",
    "DATA_DOWNLOAD_DIR = \"/dli/task/data\"\n",
    "\n",
    "# This will download the SST-2 dataset\n",
    "download_sst2(DATA_DOWNLOAD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /dli/task/data/SST-2.zip\n",
      "   creating: /dli/task/data/SST-2/\n",
      "  inflating: /dli/task/data/SST-2/dev.tsv  \n",
      "   creating: /dli/task/data/SST-2/original/\n",
      "  inflating: /dli/task/data/SST-2/original/README.txt  \n",
      "  inflating: /dli/task/data/SST-2/original/SOStr.txt  \n",
      "  inflating: /dli/task/data/SST-2/original/STree.txt  \n",
      "  inflating: /dli/task/data/SST-2/original/datasetSentences.txt  \n",
      "  inflating: /dli/task/data/SST-2/original/datasetSplit.txt  \n",
      "  inflating: /dli/task/data/SST-2/original/dictionary.txt  \n",
      "  inflating: /dli/task/data/SST-2/original/original_rt_snippets.txt  \n",
      "  inflating: /dli/task/data/SST-2/original/sentiment_labels.txt  \n",
      "  inflating: /dli/task/data/SST-2/test.tsv  \n",
      "  inflating: /dli/task/data/SST-2/train.tsv  \n"
     ]
    }
   ],
   "source": [
    "!unzip -o $DATA_DOWNLOAD_DIR/SST-2.zip -d {DATA_DOWNLOAD_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the cell below, $DATA_DIR/SST-2 will contain the following 3 files and 1 folder:\n",
    "- dev.tsv\n",
    "- test.tsv\n",
    "- train.tsv\n",
    "- original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.tsv  original  test.tsv  train.tsv\n"
     ]
    }
   ],
   "source": [
    "# Verify that the data is present\n",
    "!ls $DATA_DOWNLOAD_DIR/SST-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAO workflow\n",
    "The rest of the notebook shows what a sample TAO workflow looks like.\n",
    "\n",
    "### Setting TAO Mounts\n",
    "\n",
    "Now that our dataset has been downloaded, an important step in using TAO is to set up the directory mounts. The TAO launcher uses docker containers under the hood, and **for our data and results directory to be visible to the docker, they need to be mapped**. \n",
    "\n",
    "The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the Environment Variables and amount of Shared Memory available to the TAO launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The code below creates a sample `.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results and cache. You should configure it for your specific case such your these directories are correctly visible to the docker container. **Please also ensure that the source directories exist on your machine!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"Mounts\":[\n",
      "       {\n",
      "           \"source\": \"/dli/task/data\",\n",
      "           \"destination\": \"/data\"\n",
      "       },\n",
      "       {\n",
      "           \"source\": \"/dli/task/specs\",\n",
      "           \"destination\": \"/specs\"\n",
      "       },\n",
      "       {\n",
      "           \"source\": \"/dli/task/results\",\n",
      "           \"destination\": \"/results\"\n",
      "       },\n",
      "       {\n",
      "           \"source\": \"/root/.cache\",\n",
      "           \"destination\": \"/root/.cache\"\n",
      "       }\n",
      "   ],\n",
      "    \"DockerOptions\":{\n",
      "        \"shm_size\": \"16G\",\n",
      "        \"network\": \"host\",\n",
      "        \"ulimits\": {\n",
      "            \"memlock\": -1,\n",
      "            \"stack\": 67108864\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tee ~/.tao_mounts.json <<'EOF'\n",
    "{\n",
    "   \"Mounts\":[\n",
    "       {\n",
    "           \"source\": \"/dli/task/data\",\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"/dli/task/specs\",\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"/dli/task/results\",\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"/root/.cache\",\n",
    "           \"destination\": \"/root/.cache\"\n",
    "       }\n",
    "   ],\n",
    "    \"DockerOptions\":{\n",
    "        \"shm_size\": \"16G\",\n",
    "        \"network\": \"host\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "        }\n",
    "    }\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the source directories exist, if not, create them\n",
    "!mkdir -p \"/dli/task/specs\"\n",
    "!mkdir -p \"/dli/task/results\"\n",
    "!mkdir -p \"/root/.cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the notebook exemplifies the simplicity of the TAO workflow. \n",
    "\n",
    "Users with basic knowledge of Deep Learning can get started building their own custom models using a simple specification file. It's essentially just one command each to run data preprocessing, training, fine-tuning, evaluation, inference, and export! All configurations happen through YAML spec files. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration/Specification Files\n",
    "\n",
    "The essence of all commands in TAO lies in the **YAML specification files**. There are sample specification files already available for you to use directly or as reference to create your own.\n",
    "\n",
    "Through these specification files, you can tune many knobs like the model, dataset, hyperparameters, optimizers, etc.\n",
    "\n",
    "Each command (like download_and_convert, train, finetune, evaluate, infer, etc.) should have a dedicated specification file with configurations pertinent to it. \n",
    "\n",
    "Here is an example of the training spec file:\n",
    "\n",
    "\n",
    "```\n",
    "trainer:\n",
    "  max_epochs: 100\n",
    "\n",
    "# Name of the .tlt file where trained model will be saved.\n",
    "save_to: trained-model.tlt\n",
    "\n",
    "model:\n",
    "  # Labels that will be used to \"decode\" predictions.\n",
    "  labels:\n",
    "    \"0\": \"negative\"\n",
    "    \"1\": \"positive\"\n",
    "\n",
    "  tokenizer:\n",
    "      tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece\n",
    "      vocab_file: null # path to vocab file \n",
    "      tokenizer_model: null # only used if tokenizer is sentencepiece\n",
    "      special_tokens: null\n",
    "\n",
    " language_model:\n",
    "    pretrained_model_name: bert-base-uncased\n",
    "    lm_checkpoint: null\n",
    "    config_file: null # json file, precedence over config\n",
    "    config: null \n",
    "\n",
    "  classifier_head:\n",
    "    # This comes directly from number of labels/target classes.\n",
    "    num_output_layers: 2\n",
    "    fc_dropout: 0.1\n",
    "\n",
    "\n",
    "training_ds:\n",
    "  file_path: ???\n",
    "  batch_size: 64\n",
    "  shuffle: true\n",
    "  num_samples: -1 # number of samples to be considered, -1 means all the dataset\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting relevant paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TAO Docker. \n",
    "\n",
    "# The data is saved here\n",
    "DATA_DIR = '/data'\n",
    "\n",
    "# The configuration files are stored here\n",
    "SPECS_DIR = '/specs/text_classification'\n",
    "\n",
    "# The results are saved at this path\n",
    "RESULTS_DIR = '/results/text_classification'\n",
    "\n",
    "# Set your encryption key, and use the same key for all commands\n",
    "KEY = 'tlt_encode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is set up, we would like to take a bit of time to explain the tao interface for ease of use. The command structure can be broken down as follows: `tao <task name> <subcommand>` <br> \n",
    "\n",
    "Let's see this in further detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Specs\n",
    "We can proceed to downloading the spec files. The user may choose to modify/rewrite these specs, or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command. <br>\n",
    "\n",
    "The -o argument indicating the folder where the default specification files will be downloaded, and -r that instructs the script where to save the logs. **Make sure the -o points to an empty folder!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 08:21:30,362 [INFO] root: Registry: ['nvcr.io']\n",
      "2023-03-07 08:21:30,502 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2023-03-07 08:21:30,503 [INFO] tlt.components.docker_handler.docker_handler: The required docker doesn't exist locally/the manifest has changed. Pulling a new docker.\n",
      "2023-03-07 08:21:30,503 [INFO] tlt.components.docker_handler.docker_handler: Pulling the required container. This may take several minutes if you're doing this for the first time. Please wait here.\n",
      "...\n",
      "Pulling from repository: nvcr.io/nvidia/tao/tao-toolkit-pyt\n",
      "2023-03-07 08:25:17,845 [INFO] tlt.components.docker_handler.docker_handler: Container pull complete.\n",
      "2023-03-07 08:25:17,846 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2023-03-07 08:25:23 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:25:23 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:25:23 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:25:23 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2023-03-07 08:25:24 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2023-03-07 08:25:24 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2023-03-07 08:25:25 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:25:25 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:25:25 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo I 2023-03-07 08:25:28 tlt_logging:20] Experiment configuration:\n",
      "    exp_manager:\n",
      "      task_name: download_specs\n",
      "      explicit_log_dir: /results/text_classification\n",
      "    source_data_dir: /opt/conda/lib/python3.8/site-packages/nlp/text_classification/experiment_specs\n",
      "    target_data_dir: /specs/text_classification\n",
      "    workflow: nlp\n",
      "    \n",
      "[NeMo I 2023-03-07 08:25:28 download_specs:73] Default specification files for nlp downloaded to '/specs/text_classification'\n",
      "[NeMo I 2023-03-07 08:25:28 download_specs:74] Experiment logs saved to '/results/text_classification'\n",
      "\u001b[0m2023-03-07 08:25:29,645 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "!tao text_classification download_specs \\\n",
    "    -r $RESULTS_DIR \\\n",
    "    -o $SPECS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset-convert'></a>\n",
    "### Dataset Convert\n",
    "\n",
    "The dataset conversion feature currently supports SST-2 and IMDB dataset. You need to specify the following parameters in the command:\n",
    "\n",
    "- dataset_name: \"sst2\" or \"imdb\"\n",
    "- source_data_dir: directory path for the downloaded dataset\n",
    "- target_data_dir: directory path for the processed dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 08:25:30,635 [INFO] root: Registry: ['nvcr.io']\n",
      "2023-03-07 08:25:30,745 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2023-03-07 08:25:30,759 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2023-03-07 08:25:36 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:25:36 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:25:36 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:25:36 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2023-03-07 08:25:36 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2023-03-07 08:25:36 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2023-03-07 08:25:37 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:25:37 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:25:37 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:25:40 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/nlp/text_classification/scripts/dataset_convert.py:177: UserWarning: \n",
      "    'dataset_convert.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "[NeMo I 2023-03-07 08:25:40 tlt_logging:20] Experiment configuration:\n",
      "    dataset_name: sst2\n",
      "    source_data_dir: /data/SST-2\n",
      "    target_data_dir: /data/sst2\n",
      "    do_lower_case: true\n",
      "    exp_manager:\n",
      "      task_name: dataset_convert\n",
      "      explicit_log_dir: /specs/text_classification/dataset_convert\n",
      "    \n",
      "[NeMo I 2023-03-07 08:25:40 dataset_convert:45] Processing SST-2 dataset\n",
      "[NeMo I 2023-03-07 08:25:40 dataset_convert:80] Result stored in '/data/sst2'\n",
      "[NeMo I 2023-03-07 08:25:40 dataset_convert:173] Experiment logs saved to '/specs/text_classification/dataset_convert'\n",
      "\u001b[0m2023-03-07 08:25:41,833 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# 1. For BERT dataset conversion:\n",
    "!tao text_classification dataset_convert \\\n",
    "    -e $SPECS_DIR/dataset_convert.yaml \\\n",
    "    -r $SPECS_DIR/dataset_convert \\\n",
    "    dataset_name=sst2 source_data_dir=$DATA_DIR/SST-2 target_data_dir=$DATA_DIR/sst2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training'></a>\n",
    "### Training\n",
    "\n",
    "Training a model using TAO is as simple as configuring your spec file and running the train command. The code cell below uses the default train.yaml available for users as reference. It is configured by default to use the `bert-base-uncased` pretrained model. Additionally, these configurations could easily be overridden using the tao-launcher CLI as shown below. For instance, below we override the `training_ds.file_path`, `validation_ds.file_path`, and `trainer.max_epochs` configurations to suit our needs. We encourage you to take a look at the .yaml spec files we provide! <br>\n",
    "\n",
    "In order to get good results, you need to train for 20-50 epochs (depends on the size of the data). Training with 1 epoch in the tutorial is just for demonstration purposes.\n",
    "\n",
    "\n",
    "For training a Text Classification model in TAO, we use the `tao text_classification train` command with the following args:\n",
    "- `-e`: Path to the spec file\n",
    "- `-g`: Number of GPUs to use\n",
    "- `-k`: User specified encryption key to use while saving/loading the model\n",
    "- `-r`: Path to a folder where the outputs should be written. Make sure this is mapped in tao_mounts.json\n",
    "- Any overrides to the spec file eg. trainer.max_epochs\n",
    "\n",
    "More details about these arguments are present in the [TAO Getting Started Guide](https://docs.nvidia.com/tao/tao-toolkit/index.html) <br>\n",
    "`NOTE:` All file paths corresponds to the destination mounted directory that is visible in the TAO docker container used in backend.<br>\n",
    "\n",
    "Also worth noting is that the first time you run training on the dataset, it will run pre-processing and save that processed data in the same directory as the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 08:25:42,770 [INFO] root: Registry: ['nvcr.io']\n",
      "2023-03-07 08:25:42,874 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2023-03-07 08:25:42,887 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2023-03-07 08:25:47 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:25:47 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:25:47 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:25:47 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2023-03-07 08:25:48 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2023-03-07 08:25:48 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2023-03-07 08:25:48 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:25:48 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:25:48 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:25:53 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:25:53 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:25:53 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:25:53 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2023-03-07 08:25:53 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2023-03-07 08:25:53 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:25:54 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:25:54 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:25:54 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:25:55 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/nlp/text_classification/scripts/train.py:139: UserWarning: \n",
      "    'train.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "[NeMo I 2023-03-07 08:25:55 tlt_logging:20] Experiment configuration:\n",
      "    restore_from: ???\n",
      "    exp_manager:\n",
      "      explicit_log_dir: /results/text_classification/train\n",
      "      exp_dir: null\n",
      "      name: trained-model\n",
      "      version: null\n",
      "      use_datetime_version: true\n",
      "      resume_if_exists: true\n",
      "      resume_past_end: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_tensorboard_logger: false\n",
      "      summary_writer_kwargs: null\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs: null\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        filepath: null\n",
      "        dirpath: null\n",
      "        filename: null\n",
      "        monitor: val_loss\n",
      "        verbose: true\n",
      "        save_last: true\n",
      "        save_top_k: 3\n",
      "        save_weights_only: false\n",
      "        mode: min\n",
      "        period: null\n",
      "        every_n_val_epochs: 1\n",
      "        prefix: null\n",
      "        postfix: .tlt\n",
      "        save_best_model: false\n",
      "        always_save_nemo: false\n",
      "      files_to_copy: null\n",
      "    model:\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: null\n",
      "        tokenizer_model: null\n",
      "        special_tokens: null\n",
      "      language_model:\n",
      "        pretrained_model_name: bert-base-uncased\n",
      "        lm_checkpoint: null\n",
      "        config_file: null\n",
      "        config: null\n",
      "      classifier_head:\n",
      "        num_output_layers: 2\n",
      "        fc_dropout: 0.1\n",
      "      class_labels:\n",
      "        class_labels_file: /data/sst2/label_ids.csv\n",
      "      dataset:\n",
      "        num_classes: 2\n",
      "        do_lower_case: false\n",
      "        max_seq_length: 256\n",
      "        class_balancing: null\n",
      "        use_cache: false\n",
      "    trainer:\n",
      "      logger: false\n",
      "      checkpoint_callback: false\n",
      "      callbacks: null\n",
      "      default_root_dir: null\n",
      "      gradient_clip_val: 0.0\n",
      "      process_position: 0\n",
      "      num_nodes: 1\n",
      "      num_processes: 1\n",
      "      gpus: 1\n",
      "      auto_select_gpus: false\n",
      "      tpu_cores: null\n",
      "      log_gpu_memory: null\n",
      "      progress_bar_refresh_rate: 1\n",
      "      overfit_batches: 0.0\n",
      "      track_grad_norm: -1\n",
      "      check_val_every_n_epoch: 1\n",
      "      fast_dev_run: false\n",
      "      accumulate_grad_batches: 1\n",
      "      max_epochs: 1\n",
      "      min_epochs: 1\n",
      "      max_steps: null\n",
      "      min_steps: null\n",
      "      limit_train_batches: 1.0\n",
      "      limit_val_batches: 1.0\n",
      "      limit_test_batches: 1.0\n",
      "      val_check_interval: 1.0\n",
      "      flush_logs_every_n_steps: 100\n",
      "      log_every_n_steps: 50\n",
      "      accelerator: ddp\n",
      "      sync_batchnorm: false\n",
      "      precision: 32\n",
      "      weights_summary: full\n",
      "      weights_save_path: null\n",
      "      num_sanity_val_steps: 2\n",
      "      truncated_bptt_steps: null\n",
      "      resume_from_checkpoint: null\n",
      "      profiler: null\n",
      "      benchmark: false\n",
      "      deterministic: false\n",
      "      reload_dataloaders_every_epoch: false\n",
      "      auto_lr_find: false\n",
      "      replace_sampler_ddp: true\n",
      "      terminate_on_nan: false\n",
      "      auto_scale_batch_size: false\n",
      "      prepare_data_per_node: true\n",
      "      amp_backend: native\n",
      "      amp_level: O0\n",
      "      plugins: null\n",
      "      move_metrics_to_cpu: false\n",
      "      multiple_trainloader_mode: max_size_cycle\n",
      "      limit_predict_batches: 1.0\n",
      "      stochastic_weight_avg: false\n",
      "      gradient_clip_algorithm: norm\n",
      "      max_time: null\n",
      "      reload_dataloaders_every_n_epochs: 0\n",
      "      ipus: null\n",
      "      devices: null\n",
      "    training_ds:\n",
      "      file_path: /data/sst2/train.tsv\n",
      "      batch_size: 64\n",
      "      shuffle: true\n",
      "      num_samples: -1\n",
      "      num_workers: 3\n",
      "      drop_last: false\n",
      "      pin_memory: false\n",
      "    validation_ds:\n",
      "      file_path: /data/sst2/dev.tsv\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_samples: -1\n",
      "      num_workers: 3\n",
      "      drop_last: false\n",
      "      pin_memory: false\n",
      "    optim:\n",
      "      name: adam\n",
      "      lr: 2.0e-05\n",
      "      betas:\n",
      "      - 0.9\n",
      "      - 0.999\n",
      "      weight_decay: 0.01\n",
      "      sched:\n",
      "        name: WarmupAnnealing\n",
      "        warmup_steps: null\n",
      "        warmup_ratio: 0.1\n",
      "        last_epoch: -1\n",
      "        monitor: val_loss\n",
      "        reduce_on_plateau: false\n",
      "    encryption_key: '******'\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[NeMo W 2023-03-07 08:25:55 exp_manager:332] There was no checkpoint folder at checkpoint_dir :/results/text_classification/train/checkpoints. Training from scratch.\n",
      "[NeMo I 2023-03-07 08:25:55 exp_manager:220] Experiments will be logged at /results/text_classification/train\n",
      "[NeMo W 2023-03-07 08:25:55 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:240: LightningDeprecationWarning: `ModelCheckpoint(every_n_val_epochs)` is deprecated in v1.4 and will be removed in v1.6. Please use `every_n_epochs` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo I 2023-03-07 08:25:55 tokenizer_utils:100] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: None, special_tokens_dict: {}, and use_fast: False\n",
      "Lock 139680870350224 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 34.7kB/s]\n",
      "Lock 139680870350224 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "Lock 139680870227200 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 765kB/s]\n",
      "Lock 139680870227200 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Lock 139680870227008 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 5.31MB/s]\n",
      "Lock 139680870227008 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Lock 139680870309216 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 8.58MB/s]\n",
      "Lock 139680870309216 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2023-03-07 08:25:56 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Lock 139680870307152 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Downloading: 100%|████████████████████████████| 440M/440M [00:04<00:00, 108MB/s]\n",
      "Lock 139680870307152 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertEncoder: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2023-03-07 08:26:03 text_classification_dataset:120] Read 67349 examples from /data/sst2/train.tsv.\n",
      "[NeMo I 2023-03-07 08:26:03 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-03-07 08:26:03 text_classification_dataset:239] example 0: ['`', 'fatal', 'script', 'error']\n",
      "[NeMo I 2023-03-07 08:26:03 text_classification_dataset:240] subtokens: [CLS] ` fatal script error [SEP]\n",
      "[NeMo I 2023-03-07 08:26:03 text_classification_dataset:241] input_ids: 101 1036 10611 5896 7561 102\n",
      "[NeMo I 2023-03-07 08:26:03 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0\n",
      "[NeMo I 2023-03-07 08:26:03 text_classification_dataset:243] input_mask: 1 1 1 1 1 1\n",
      "[NeMo I 2023-03-07 08:26:03 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2023-03-07 08:26:03 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-03-07 08:26:03 text_classification_dataset:239] example 1: ['are', 'of', 'the', 'highest', 'and', 'the', 'performances']\n",
      "[NeMo I 2023-03-07 08:26:03 text_classification_dataset:240] subtokens: [CLS] are of the highest and the performances [SEP]\n",
      "[NeMo I 2023-03-07 08:26:03 text_classification_dataset:241] input_ids: 101 2024 1997 1996 3284 1998 1996 4616 102\n",
      "[NeMo I 2023-03-07 08:26:03 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2023-03-07 08:26:03 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2023-03-07 08:26:03 text_classification_dataset:244] label: 1\n",
      "[NeMo I 2023-03-07 08:26:39 data_preprocessing:358] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-03-07 08:26:39 data_preprocessing:360] Min: 3 |                  Max: 66 |                  Mean: 13.319262349849293 |                  Median: 10.0\n",
      "[NeMo I 2023-03-07 08:26:39 data_preprocessing:366] 75 percentile: 18.00\n",
      "[NeMo I 2023-03-07 08:26:39 data_preprocessing:367] 99 percentile: 44.00\n",
      "[NeMo I 2023-03-07 08:26:41 text_classification_dataset:120] Read 872 examples from /data/sst2/dev.tsv.\n",
      "[NeMo I 2023-03-07 08:26:41 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-03-07 08:26:41 text_classification_dataset:239] example 0: ['it', \"'s\", 'a', 'charming', 'and', 'often', 'affecting', 'journey', '.']\n",
      "[NeMo I 2023-03-07 08:26:41 text_classification_dataset:240] subtokens: [CLS] it ' s a charming and often affecting journey . [SEP]\n",
      "[NeMo I 2023-03-07 08:26:41 text_classification_dataset:241] input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102\n",
      "[NeMo I 2023-03-07 08:26:41 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2023-03-07 08:26:41 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2023-03-07 08:26:41 text_classification_dataset:244] label: 1\n",
      "[NeMo I 2023-03-07 08:26:41 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-03-07 08:26:41 text_classification_dataset:239] example 1: ['unflinchingly', 'bleak', 'and', 'desperate']\n",
      "[NeMo I 2023-03-07 08:26:41 text_classification_dataset:240] subtokens: [CLS] un ##fl ##in ##ching ##ly bleak and desperate [SEP]\n",
      "[NeMo I 2023-03-07 08:26:41 text_classification_dataset:241] input_ids: 101 4895 10258 2378 8450 2135 21657 1998 7143 102\n",
      "[NeMo I 2023-03-07 08:26:41 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2023-03-07 08:26:41 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2023-03-07 08:26:41 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2023-03-07 08:26:42 data_preprocessing:358] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-03-07 08:26:42 data_preprocessing:360] Min: 4 |                  Max: 55 |                  Mean: 25.163990825688074 |                  Median: 24.5\n",
      "[NeMo I 2023-03-07 08:26:42 data_preprocessing:366] 75 percentile: 32.00\n",
      "[NeMo I 2023-03-07 08:26:42 data_preprocessing:367] 99 percentile: 51.29\n",
      "[NeMo I 2023-03-07 08:26:42 modelPT:544] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.999]\n",
      "        eps: 1e-08\n",
      "        lr: 2e-05\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2023-03-07 08:26:42 lr_scheduler:621] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7f09e7ccc670>\" \n",
      "    will be used during training (effective maximum steps = 1053) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 1053\n",
      "    )\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for 1 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All DDP processes registered. Starting ddp with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2023-03-07 08:26:44 modelPT:415] No optimizer config provided, therefore no optimizer was created\n",
      "\n",
      "    | Name                                                   | Type                 | Params\n",
      "--------------------------------------------------------------------------------------------------\n",
      "0   | bert_model                                             | BertEncoder          | 109 M \n",
      "1   | bert_model.embeddings                                  | BertEmbeddings       | 23.8 M\n",
      "2   | bert_model.embeddings.word_embeddings                  | Embedding            | 23.4 M\n",
      "3   | bert_model.embeddings.position_embeddings              | Embedding            | 393 K \n",
      "4   | bert_model.embeddings.token_type_embeddings            | Embedding            | 1.5 K \n",
      "5   | bert_model.embeddings.LayerNorm                        | LayerNorm            | 1.5 K \n",
      "6   | bert_model.embeddings.dropout                          | Dropout              | 0     \n",
      "7   | bert_model.encoder                                     | BertEncoder          | 85.1 M\n",
      "8   | bert_model.encoder.layer                               | ModuleList           | 85.1 M\n",
      "9   | bert_model.encoder.layer.0                             | BertLayer            | 7.1 M \n",
      "10  | bert_model.encoder.layer.0.attention                   | BertAttention        | 2.4 M \n",
      "11  | bert_model.encoder.layer.0.attention.self              | BertSelfAttention    | 1.8 M \n",
      "12  | bert_model.encoder.layer.0.attention.self.query        | Linear               | 590 K \n",
      "13  | bert_model.encoder.layer.0.attention.self.key          | Linear               | 590 K \n",
      "14  | bert_model.encoder.layer.0.attention.self.value        | Linear               | 590 K \n",
      "15  | bert_model.encoder.layer.0.attention.self.dropout      | Dropout              | 0     \n",
      "16  | bert_model.encoder.layer.0.attention.output            | BertSelfOutput       | 592 K \n",
      "17  | bert_model.encoder.layer.0.attention.output.dense      | Linear               | 590 K \n",
      "18  | bert_model.encoder.layer.0.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "19  | bert_model.encoder.layer.0.attention.output.dropout    | Dropout              | 0     \n",
      "20  | bert_model.encoder.layer.0.intermediate                | BertIntermediate     | 2.4 M \n",
      "21  | bert_model.encoder.layer.0.intermediate.dense          | Linear               | 2.4 M \n",
      "22  | bert_model.encoder.layer.0.output                      | BertOutput           | 2.4 M \n",
      "23  | bert_model.encoder.layer.0.output.dense                | Linear               | 2.4 M \n",
      "24  | bert_model.encoder.layer.0.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "25  | bert_model.encoder.layer.0.output.dropout              | Dropout              | 0     \n",
      "26  | bert_model.encoder.layer.1                             | BertLayer            | 7.1 M \n",
      "27  | bert_model.encoder.layer.1.attention                   | BertAttention        | 2.4 M \n",
      "28  | bert_model.encoder.layer.1.attention.self              | BertSelfAttention    | 1.8 M \n",
      "29  | bert_model.encoder.layer.1.attention.self.query        | Linear               | 590 K \n",
      "30  | bert_model.encoder.layer.1.attention.self.key          | Linear               | 590 K \n",
      "31  | bert_model.encoder.layer.1.attention.self.value        | Linear               | 590 K \n",
      "32  | bert_model.encoder.layer.1.attention.self.dropout      | Dropout              | 0     \n",
      "33  | bert_model.encoder.layer.1.attention.output            | BertSelfOutput       | 592 K \n",
      "34  | bert_model.encoder.layer.1.attention.output.dense      | Linear               | 590 K \n",
      "35  | bert_model.encoder.layer.1.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "36  | bert_model.encoder.layer.1.attention.output.dropout    | Dropout              | 0     \n",
      "37  | bert_model.encoder.layer.1.intermediate                | BertIntermediate     | 2.4 M \n",
      "38  | bert_model.encoder.layer.1.intermediate.dense          | Linear               | 2.4 M \n",
      "39  | bert_model.encoder.layer.1.output                      | BertOutput           | 2.4 M \n",
      "40  | bert_model.encoder.layer.1.output.dense                | Linear               | 2.4 M \n",
      "41  | bert_model.encoder.layer.1.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "42  | bert_model.encoder.layer.1.output.dropout              | Dropout              | 0     \n",
      "43  | bert_model.encoder.layer.2                             | BertLayer            | 7.1 M \n",
      "44  | bert_model.encoder.layer.2.attention                   | BertAttention        | 2.4 M \n",
      "45  | bert_model.encoder.layer.2.attention.self              | BertSelfAttention    | 1.8 M \n",
      "46  | bert_model.encoder.layer.2.attention.self.query        | Linear               | 590 K \n",
      "47  | bert_model.encoder.layer.2.attention.self.key          | Linear               | 590 K \n",
      "48  | bert_model.encoder.layer.2.attention.self.value        | Linear               | 590 K \n",
      "49  | bert_model.encoder.layer.2.attention.self.dropout      | Dropout              | 0     \n",
      "50  | bert_model.encoder.layer.2.attention.output            | BertSelfOutput       | 592 K \n",
      "51  | bert_model.encoder.layer.2.attention.output.dense      | Linear               | 590 K \n",
      "52  | bert_model.encoder.layer.2.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "53  | bert_model.encoder.layer.2.attention.output.dropout    | Dropout              | 0     \n",
      "54  | bert_model.encoder.layer.2.intermediate                | BertIntermediate     | 2.4 M \n",
      "55  | bert_model.encoder.layer.2.intermediate.dense          | Linear               | 2.4 M \n",
      "56  | bert_model.encoder.layer.2.output                      | BertOutput           | 2.4 M \n",
      "57  | bert_model.encoder.layer.2.output.dense                | Linear               | 2.4 M \n",
      "58  | bert_model.encoder.layer.2.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "59  | bert_model.encoder.layer.2.output.dropout              | Dropout              | 0     \n",
      "60  | bert_model.encoder.layer.3                             | BertLayer            | 7.1 M \n",
      "61  | bert_model.encoder.layer.3.attention                   | BertAttention        | 2.4 M \n",
      "62  | bert_model.encoder.layer.3.attention.self              | BertSelfAttention    | 1.8 M \n",
      "63  | bert_model.encoder.layer.3.attention.self.query        | Linear               | 590 K \n",
      "64  | bert_model.encoder.layer.3.attention.self.key          | Linear               | 590 K \n",
      "65  | bert_model.encoder.layer.3.attention.self.value        | Linear               | 590 K \n",
      "66  | bert_model.encoder.layer.3.attention.self.dropout      | Dropout              | 0     \n",
      "67  | bert_model.encoder.layer.3.attention.output            | BertSelfOutput       | 592 K \n",
      "68  | bert_model.encoder.layer.3.attention.output.dense      | Linear               | 590 K \n",
      "69  | bert_model.encoder.layer.3.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "70  | bert_model.encoder.layer.3.attention.output.dropout    | Dropout              | 0     \n",
      "71  | bert_model.encoder.layer.3.intermediate                | BertIntermediate     | 2.4 M \n",
      "72  | bert_model.encoder.layer.3.intermediate.dense          | Linear               | 2.4 M \n",
      "73  | bert_model.encoder.layer.3.output                      | BertOutput           | 2.4 M \n",
      "74  | bert_model.encoder.layer.3.output.dense                | Linear               | 2.4 M \n",
      "75  | bert_model.encoder.layer.3.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "76  | bert_model.encoder.layer.3.output.dropout              | Dropout              | 0     \n",
      "77  | bert_model.encoder.layer.4                             | BertLayer            | 7.1 M \n",
      "78  | bert_model.encoder.layer.4.attention                   | BertAttention        | 2.4 M \n",
      "79  | bert_model.encoder.layer.4.attention.self              | BertSelfAttention    | 1.8 M \n",
      "80  | bert_model.encoder.layer.4.attention.self.query        | Linear               | 590 K \n",
      "81  | bert_model.encoder.layer.4.attention.self.key          | Linear               | 590 K \n",
      "82  | bert_model.encoder.layer.4.attention.self.value        | Linear               | 590 K \n",
      "83  | bert_model.encoder.layer.4.attention.self.dropout      | Dropout              | 0     \n",
      "84  | bert_model.encoder.layer.4.attention.output            | BertSelfOutput       | 592 K \n",
      "85  | bert_model.encoder.layer.4.attention.output.dense      | Linear               | 590 K \n",
      "86  | bert_model.encoder.layer.4.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "87  | bert_model.encoder.layer.4.attention.output.dropout    | Dropout              | 0     \n",
      "88  | bert_model.encoder.layer.4.intermediate                | BertIntermediate     | 2.4 M \n",
      "89  | bert_model.encoder.layer.4.intermediate.dense          | Linear               | 2.4 M \n",
      "90  | bert_model.encoder.layer.4.output                      | BertOutput           | 2.4 M \n",
      "91  | bert_model.encoder.layer.4.output.dense                | Linear               | 2.4 M \n",
      "92  | bert_model.encoder.layer.4.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "93  | bert_model.encoder.layer.4.output.dropout              | Dropout              | 0     \n",
      "94  | bert_model.encoder.layer.5                             | BertLayer            | 7.1 M \n",
      "95  | bert_model.encoder.layer.5.attention                   | BertAttention        | 2.4 M \n",
      "96  | bert_model.encoder.layer.5.attention.self              | BertSelfAttention    | 1.8 M \n",
      "97  | bert_model.encoder.layer.5.attention.self.query        | Linear               | 590 K \n",
      "98  | bert_model.encoder.layer.5.attention.self.key          | Linear               | 590 K \n",
      "99  | bert_model.encoder.layer.5.attention.self.value        | Linear               | 590 K \n",
      "100 | bert_model.encoder.layer.5.attention.self.dropout      | Dropout              | 0     \n",
      "101 | bert_model.encoder.layer.5.attention.output            | BertSelfOutput       | 592 K \n",
      "102 | bert_model.encoder.layer.5.attention.output.dense      | Linear               | 590 K \n",
      "103 | bert_model.encoder.layer.5.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "104 | bert_model.encoder.layer.5.attention.output.dropout    | Dropout              | 0     \n",
      "105 | bert_model.encoder.layer.5.intermediate                | BertIntermediate     | 2.4 M \n",
      "106 | bert_model.encoder.layer.5.intermediate.dense          | Linear               | 2.4 M \n",
      "107 | bert_model.encoder.layer.5.output                      | BertOutput           | 2.4 M \n",
      "108 | bert_model.encoder.layer.5.output.dense                | Linear               | 2.4 M \n",
      "109 | bert_model.encoder.layer.5.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "110 | bert_model.encoder.layer.5.output.dropout              | Dropout              | 0     \n",
      "111 | bert_model.encoder.layer.6                             | BertLayer            | 7.1 M \n",
      "112 | bert_model.encoder.layer.6.attention                   | BertAttention        | 2.4 M \n",
      "113 | bert_model.encoder.layer.6.attention.self              | BertSelfAttention    | 1.8 M \n",
      "114 | bert_model.encoder.layer.6.attention.self.query        | Linear               | 590 K \n",
      "115 | bert_model.encoder.layer.6.attention.self.key          | Linear               | 590 K \n",
      "116 | bert_model.encoder.layer.6.attention.self.value        | Linear               | 590 K \n",
      "117 | bert_model.encoder.layer.6.attention.self.dropout      | Dropout              | 0     \n",
      "118 | bert_model.encoder.layer.6.attention.output            | BertSelfOutput       | 592 K \n",
      "119 | bert_model.encoder.layer.6.attention.output.dense      | Linear               | 590 K \n",
      "120 | bert_model.encoder.layer.6.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "121 | bert_model.encoder.layer.6.attention.output.dropout    | Dropout              | 0     \n",
      "122 | bert_model.encoder.layer.6.intermediate                | BertIntermediate     | 2.4 M \n",
      "123 | bert_model.encoder.layer.6.intermediate.dense          | Linear               | 2.4 M \n",
      "124 | bert_model.encoder.layer.6.output                      | BertOutput           | 2.4 M \n",
      "125 | bert_model.encoder.layer.6.output.dense                | Linear               | 2.4 M \n",
      "126 | bert_model.encoder.layer.6.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "127 | bert_model.encoder.layer.6.output.dropout              | Dropout              | 0     \n",
      "128 | bert_model.encoder.layer.7                             | BertLayer            | 7.1 M \n",
      "129 | bert_model.encoder.layer.7.attention                   | BertAttention        | 2.4 M \n",
      "130 | bert_model.encoder.layer.7.attention.self              | BertSelfAttention    | 1.8 M \n",
      "131 | bert_model.encoder.layer.7.attention.self.query        | Linear               | 590 K \n",
      "132 | bert_model.encoder.layer.7.attention.self.key          | Linear               | 590 K \n",
      "133 | bert_model.encoder.layer.7.attention.self.value        | Linear               | 590 K \n",
      "134 | bert_model.encoder.layer.7.attention.self.dropout      | Dropout              | 0     \n",
      "135 | bert_model.encoder.layer.7.attention.output            | BertSelfOutput       | 592 K \n",
      "136 | bert_model.encoder.layer.7.attention.output.dense      | Linear               | 590 K \n",
      "137 | bert_model.encoder.layer.7.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "138 | bert_model.encoder.layer.7.attention.output.dropout    | Dropout              | 0     \n",
      "139 | bert_model.encoder.layer.7.intermediate                | BertIntermediate     | 2.4 M \n",
      "140 | bert_model.encoder.layer.7.intermediate.dense          | Linear               | 2.4 M \n",
      "141 | bert_model.encoder.layer.7.output                      | BertOutput           | 2.4 M \n",
      "142 | bert_model.encoder.layer.7.output.dense                | Linear               | 2.4 M \n",
      "143 | bert_model.encoder.layer.7.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "144 | bert_model.encoder.layer.7.output.dropout              | Dropout              | 0     \n",
      "145 | bert_model.encoder.layer.8                             | BertLayer            | 7.1 M \n",
      "146 | bert_model.encoder.layer.8.attention                   | BertAttention        | 2.4 M \n",
      "147 | bert_model.encoder.layer.8.attention.self              | BertSelfAttention    | 1.8 M \n",
      "148 | bert_model.encoder.layer.8.attention.self.query        | Linear               | 590 K \n",
      "149 | bert_model.encoder.layer.8.attention.self.key          | Linear               | 590 K \n",
      "150 | bert_model.encoder.layer.8.attention.self.value        | Linear               | 590 K \n",
      "151 | bert_model.encoder.layer.8.attention.self.dropout      | Dropout              | 0     \n",
      "152 | bert_model.encoder.layer.8.attention.output            | BertSelfOutput       | 592 K \n",
      "153 | bert_model.encoder.layer.8.attention.output.dense      | Linear               | 590 K \n",
      "154 | bert_model.encoder.layer.8.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "155 | bert_model.encoder.layer.8.attention.output.dropout    | Dropout              | 0     \n",
      "156 | bert_model.encoder.layer.8.intermediate                | BertIntermediate     | 2.4 M \n",
      "157 | bert_model.encoder.layer.8.intermediate.dense          | Linear               | 2.4 M \n",
      "158 | bert_model.encoder.layer.8.output                      | BertOutput           | 2.4 M \n",
      "159 | bert_model.encoder.layer.8.output.dense                | Linear               | 2.4 M \n",
      "160 | bert_model.encoder.layer.8.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "161 | bert_model.encoder.layer.8.output.dropout              | Dropout              | 0     \n",
      "162 | bert_model.encoder.layer.9                             | BertLayer            | 7.1 M \n",
      "163 | bert_model.encoder.layer.9.attention                   | BertAttention        | 2.4 M \n",
      "164 | bert_model.encoder.layer.9.attention.self              | BertSelfAttention    | 1.8 M \n",
      "165 | bert_model.encoder.layer.9.attention.self.query        | Linear               | 590 K \n",
      "166 | bert_model.encoder.layer.9.attention.self.key          | Linear               | 590 K \n",
      "167 | bert_model.encoder.layer.9.attention.self.value        | Linear               | 590 K \n",
      "168 | bert_model.encoder.layer.9.attention.self.dropout      | Dropout              | 0     \n",
      "169 | bert_model.encoder.layer.9.attention.output            | BertSelfOutput       | 592 K \n",
      "170 | bert_model.encoder.layer.9.attention.output.dense      | Linear               | 590 K \n",
      "171 | bert_model.encoder.layer.9.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "172 | bert_model.encoder.layer.9.attention.output.dropout    | Dropout              | 0     \n",
      "173 | bert_model.encoder.layer.9.intermediate                | BertIntermediate     | 2.4 M \n",
      "174 | bert_model.encoder.layer.9.intermediate.dense          | Linear               | 2.4 M \n",
      "175 | bert_model.encoder.layer.9.output                      | BertOutput           | 2.4 M \n",
      "176 | bert_model.encoder.layer.9.output.dense                | Linear               | 2.4 M \n",
      "177 | bert_model.encoder.layer.9.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "178 | bert_model.encoder.layer.9.output.dropout              | Dropout              | 0     \n",
      "179 | bert_model.encoder.layer.10                            | BertLayer            | 7.1 M \n",
      "180 | bert_model.encoder.layer.10.attention                  | BertAttention        | 2.4 M \n",
      "181 | bert_model.encoder.layer.10.attention.self             | BertSelfAttention    | 1.8 M \n",
      "182 | bert_model.encoder.layer.10.attention.self.query       | Linear               | 590 K \n",
      "183 | bert_model.encoder.layer.10.attention.self.key         | Linear               | 590 K \n",
      "184 | bert_model.encoder.layer.10.attention.self.value       | Linear               | 590 K \n",
      "185 | bert_model.encoder.layer.10.attention.self.dropout     | Dropout              | 0     \n",
      "186 | bert_model.encoder.layer.10.attention.output           | BertSelfOutput       | 592 K \n",
      "187 | bert_model.encoder.layer.10.attention.output.dense     | Linear               | 590 K \n",
      "188 | bert_model.encoder.layer.10.attention.output.LayerNorm | LayerNorm            | 1.5 K \n",
      "189 | bert_model.encoder.layer.10.attention.output.dropout   | Dropout              | 0     \n",
      "190 | bert_model.encoder.layer.10.intermediate               | BertIntermediate     | 2.4 M \n",
      "191 | bert_model.encoder.layer.10.intermediate.dense         | Linear               | 2.4 M \n",
      "192 | bert_model.encoder.layer.10.output                     | BertOutput           | 2.4 M \n",
      "193 | bert_model.encoder.layer.10.output.dense               | Linear               | 2.4 M \n",
      "194 | bert_model.encoder.layer.10.output.LayerNorm           | LayerNorm            | 1.5 K \n",
      "195 | bert_model.encoder.layer.10.output.dropout             | Dropout              | 0     \n",
      "196 | bert_model.encoder.layer.11                            | BertLayer            | 7.1 M \n",
      "197 | bert_model.encoder.layer.11.attention                  | BertAttention        | 2.4 M \n",
      "198 | bert_model.encoder.layer.11.attention.self             | BertSelfAttention    | 1.8 M \n",
      "199 | bert_model.encoder.layer.11.attention.self.query       | Linear               | 590 K \n",
      "200 | bert_model.encoder.layer.11.attention.self.key         | Linear               | 590 K \n",
      "201 | bert_model.encoder.layer.11.attention.self.value       | Linear               | 590 K \n",
      "202 | bert_model.encoder.layer.11.attention.self.dropout     | Dropout              | 0     \n",
      "203 | bert_model.encoder.layer.11.attention.output           | BertSelfOutput       | 592 K \n",
      "204 | bert_model.encoder.layer.11.attention.output.dense     | Linear               | 590 K \n",
      "205 | bert_model.encoder.layer.11.attention.output.LayerNorm | LayerNorm            | 1.5 K \n",
      "206 | bert_model.encoder.layer.11.attention.output.dropout   | Dropout              | 0     \n",
      "207 | bert_model.encoder.layer.11.intermediate               | BertIntermediate     | 2.4 M \n",
      "208 | bert_model.encoder.layer.11.intermediate.dense         | Linear               | 2.4 M \n",
      "209 | bert_model.encoder.layer.11.output                     | BertOutput           | 2.4 M \n",
      "210 | bert_model.encoder.layer.11.output.dense               | Linear               | 2.4 M \n",
      "211 | bert_model.encoder.layer.11.output.LayerNorm           | LayerNorm            | 1.5 K \n",
      "212 | bert_model.encoder.layer.11.output.dropout             | Dropout              | 0     \n",
      "213 | bert_model.pooler                                      | BertPooler           | 590 K \n",
      "214 | bert_model.pooler.dense                                | Linear               | 590 K \n",
      "215 | bert_model.pooler.activation                           | Tanh                 | 0     \n",
      "216 | classifier                                             | SequenceClassifier   | 592 K \n",
      "217 | classifier.dropout                                     | Dropout              | 0     \n",
      "218 | classifier.mlp                                         | MultiLayerPerceptron | 592 K \n",
      "219 | classifier.mlp.layer0                                  | Linear               | 590 K \n",
      "220 | classifier.mlp.layer2                                  | Linear               | 1.5 K \n",
      "221 | loss                                                   | CrossEntropyLoss     | 0     \n",
      "222 | classification_report                                  | ClassificationReport | 0     \n",
      "--------------------------------------------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.297   Total estimated model params size (MB)\n",
      "Validation sanity check: 100%|████████████████████| 2/2 [00:02<00:00,  1.54s/it][NeMo I 2023-03-07 08:26:46 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             40.00       3.28       6.06         61\n",
      "    label_id: 1                                             52.03      95.52      67.37         67\n",
      "    -------------------\n",
      "    micro avg                                               51.56      51.56      51.56        128\n",
      "    macro avg                                               46.02      49.40      36.71        128\n",
      "    weighted avg                                            46.30      51.56      38.15        128\n",
      "    \n",
      "Epoch 0:  99%|█████▉| 1053/1067 [07:33<00:06,  2.32it/s, loss=0.193, lr=2.11e-8]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  99%|█████▉| 1055/1067 [07:34<00:05,  2.33it/s, loss=0.193, lr=2.11e-8]\u001b[A\n",
      "Validating:  14%|████▌                           | 2/14 [00:00<00:02,  4.27it/s]\u001b[A\n",
      "Epoch 0:  99%|█████▉| 1057/1067 [07:34<00:04,  2.33it/s, loss=0.193, lr=2.11e-8]\u001b[A\n",
      "Validating:  29%|█████████▏                      | 4/14 [00:00<00:02,  4.87it/s]\u001b[A\n",
      "Epoch 0:  99%|█████▉| 1059/1067 [07:34<00:03,  2.33it/s, loss=0.193, lr=2.11e-8]\u001b[A\n",
      "Validating:  43%|█████████████▋                  | 6/14 [00:01<00:01,  5.33it/s]\u001b[A\n",
      "Epoch 0:  99%|█████▉| 1061/1067 [07:35<00:02,  2.33it/s, loss=0.193, lr=2.11e-8]\u001b[A\n",
      "Validating:  57%|██████████████████▎             | 8/14 [00:01<00:01,  5.64it/s]\u001b[A\n",
      "Epoch 0: 100%|█████▉| 1063/1067 [07:35<00:01,  2.34it/s, loss=0.193, lr=2.11e-8]\u001b[A\n",
      "Validating:  71%|██████████████████████▏        | 10/14 [00:01<00:00,  5.77it/s]\u001b[A\n",
      "Epoch 0: 100%|█████▉| 1065/1067 [07:35<00:00,  2.34it/s, loss=0.193, lr=2.11e-8]\u001b[A\n",
      "Validating:  86%|██████████████████████████▌    | 12/14 [00:02<00:00,  5.84it/s]\u001b[A\n",
      "Epoch 0: 100%|██████| 1067/1067 [07:36<00:00,  2.34it/s, loss=0.193, lr=2.11e-8]\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 14/14 [00:02<00:00,  6.44it/s]\u001b[A[NeMo I 2023-03-07 08:34:23 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             91.83      89.25      90.52        428\n",
      "    label_id: 1                                             89.91      92.34      91.11        444\n",
      "    -------------------\n",
      "    micro avg                                               90.83      90.83      90.83        872\n",
      "    macro avg                                               90.87      90.80      90.82        872\n",
      "    weighted avg                                            90.85      90.83      90.82        872\n",
      "    \n",
      "Epoch 0: 100%|█| 1067/1067 [07:36<00:00,  2.34it/s, loss=0.193, lr=2.11e-8, val_\n",
      "                                                                                \u001b[AEpoch 0, global step 1052: val_loss reached 0.23900 (best 0.23900), saving model to \"/results/text_classification/train/checkpoints/trained-model--val_loss=0.24-epoch=0.ckpt\" as top 3\n",
      "Epoch 0: 100%|█| 1067/1067 [08:00<00:00,  2.22it/s, loss=0.193, lr=2.11e-8, val_\n",
      "[NeMo I 2023-03-07 08:35:07 train:130] Experiment logs saved to '/results/text_classification/train'\n",
      "[NeMo I 2023-03-07 08:35:07 train:131] Trained model saved to '/results/text_classification/train/checkpoints/trained-model.tlt'\n",
      "\u001b[0m\u001b[0m2023-03-07 08:35:09,622 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# 2. For BERT training on SST-2:\n",
    "!tao text_classification train \\\n",
    "    -e $SPECS_DIR/train.yaml \\\n",
    "    -g 1  \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/train \\\n",
    "    training_ds.file_path=$DATA_DIR/sst2/train.tsv \\\n",
    "    validation_ds.file_path=$DATA_DIR/sst2/dev.tsv \\\n",
    "    model.class_labels.class_labels_file=$DATA_DIR/sst2/label_ids.csv \\\n",
    "    trainer.max_epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train command produces a .tlt file called `trained-model.tlt` saved at `$RESULTS_DIR/train/checkpoints/trained-model.tlt`. This file can be fed directly into the fine-tuning stage as we see in the next block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other tips and tricks:\n",
    "- To accelerate the training without loss of quality, it is possible to train with these parameters:  `trainer.amp_level=\"O1\"` and `trainer.precision=16` for reduced precision.\n",
    "- The batch size (`training_ds.batch_size`) may influence the validation accuracy. Larger batch sizes are faster to train with, however, you may get slightly better results with smaller batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ft'></a>\n",
    "### Fine-Tuning\n",
    "\n",
    "The command for fine-tuning is very similar to that of training. Instead of `tao text_classification train`, we use `tao text_classification finetune` instead. We also specify the spec file corresponding to fine-tuning. All commands in TAO follow a similar pattern, streamlining the workflow even further!\n",
    "\n",
    "`Note:` we use SST-2 dataset here to showcase how to use the fine-tuning command, however we recommend bringing your own data for fine-tuning on the pre-trained models. You would need to make sure your dataset is downloaded and pre-processed so that it's ready for use.\n",
    "\n",
    "`Note:` If you wish to proceed with a trained dataset for better inference results, you can find a .nemo model [here](\n",
    "https://ngc.nvidia.com/catalog/collections/nvidia:nemotrainingframework).\n",
    "\n",
    "Simply re-name the .nemo file to .tlt and pass it through the finetune pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 08:35:11,187 [INFO] root: Registry: ['nvcr.io']\n",
      "2023-03-07 08:35:11,317 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2023-03-07 08:35:11,327 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2023-03-07 08:35:16 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:35:16 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:35:16 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:35:16 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2023-03-07 08:35:16 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2023-03-07 08:35:16 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2023-03-07 08:35:17 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:35:17 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:35:17 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:35:22 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:35:22 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:35:22 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:35:22 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2023-03-07 08:35:22 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2023-03-07 08:35:22 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:35:22 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:35:22 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:35:22 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:35:23 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/nlp/text_classification/scripts/finetune.py:140: UserWarning: \n",
      "    'finetune.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "[NeMo I 2023-03-07 08:35:23 tlt_logging:20] Experiment configuration:\n",
      "    restore_from: /results/text_classification/train/checkpoints/trained-model.tlt\n",
      "    exp_manager:\n",
      "      explicit_log_dir: /results/text_classification/finetune\n",
      "      exp_dir: null\n",
      "      name: finetuned-model\n",
      "      version: null\n",
      "      use_datetime_version: true\n",
      "      resume_if_exists: true\n",
      "      resume_past_end: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_tensorboard_logger: false\n",
      "      summary_writer_kwargs: null\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs: null\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        filepath: null\n",
      "        dirpath: null\n",
      "        filename: null\n",
      "        monitor: val_loss\n",
      "        verbose: true\n",
      "        save_last: true\n",
      "        save_top_k: 3\n",
      "        save_weights_only: false\n",
      "        mode: min\n",
      "        period: null\n",
      "        every_n_val_epochs: 1\n",
      "        prefix: null\n",
      "        postfix: .tlt\n",
      "        save_best_model: false\n",
      "        always_save_nemo: false\n",
      "      files_to_copy: null\n",
      "    trainer:\n",
      "      logger: false\n",
      "      checkpoint_callback: false\n",
      "      callbacks: null\n",
      "      default_root_dir: null\n",
      "      gradient_clip_val: 0.0\n",
      "      process_position: 0\n",
      "      num_nodes: 1\n",
      "      num_processes: 1\n",
      "      gpus: 1\n",
      "      auto_select_gpus: false\n",
      "      tpu_cores: null\n",
      "      log_gpu_memory: null\n",
      "      progress_bar_refresh_rate: 1\n",
      "      overfit_batches: 0.0\n",
      "      track_grad_norm: -1\n",
      "      check_val_every_n_epoch: 1\n",
      "      fast_dev_run: false\n",
      "      accumulate_grad_batches: 1\n",
      "      max_epochs: 1\n",
      "      min_epochs: 1\n",
      "      max_steps: null\n",
      "      min_steps: null\n",
      "      limit_train_batches: 1.0\n",
      "      limit_val_batches: 1.0\n",
      "      limit_test_batches: 1.0\n",
      "      val_check_interval: 1.0\n",
      "      flush_logs_every_n_steps: 100\n",
      "      log_every_n_steps: 50\n",
      "      accelerator: ddp\n",
      "      sync_batchnorm: false\n",
      "      precision: 32\n",
      "      weights_summary: full\n",
      "      weights_save_path: null\n",
      "      num_sanity_val_steps: 2\n",
      "      truncated_bptt_steps: null\n",
      "      resume_from_checkpoint: null\n",
      "      profiler: null\n",
      "      benchmark: false\n",
      "      deterministic: false\n",
      "      reload_dataloaders_every_epoch: false\n",
      "      auto_lr_find: false\n",
      "      replace_sampler_ddp: true\n",
      "      terminate_on_nan: false\n",
      "      auto_scale_batch_size: false\n",
      "      prepare_data_per_node: true\n",
      "      amp_backend: native\n",
      "      amp_level: O0\n",
      "      plugins: null\n",
      "      move_metrics_to_cpu: false\n",
      "      multiple_trainloader_mode: max_size_cycle\n",
      "      limit_predict_batches: 1.0\n",
      "      stochastic_weight_avg: false\n",
      "      gradient_clip_algorithm: norm\n",
      "      max_time: null\n",
      "      reload_dataloaders_every_n_epochs: 0\n",
      "      ipus: null\n",
      "      devices: null\n",
      "    finetuning_ds:\n",
      "      file_path: /data/sst2/train.tsv\n",
      "      batch_size: 64\n",
      "      shuffle: true\n",
      "      num_samples: -1\n",
      "      num_workers: 3\n",
      "      drop_last: false\n",
      "      pin_memory: false\n",
      "    validation_ds:\n",
      "      file_path: /data/sst2/dev.tsv\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_samples: -1\n",
      "      num_workers: 3\n",
      "      drop_last: false\n",
      "      pin_memory: false\n",
      "    optim:\n",
      "      name: adam\n",
      "      lr: 0.001\n",
      "      betas:\n",
      "      - 0.9\n",
      "      - 0.9998\n",
      "      weight_decay: 0.001\n",
      "    encryption_key: '********'\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[NeMo W 2023-03-07 08:35:23 exp_manager:332] There was no checkpoint folder at checkpoint_dir :/results/text_classification/finetune/checkpoints. Training from scratch.\n",
      "[NeMo I 2023-03-07 08:35:23 exp_manager:220] Experiments will be logged at /results/text_classification/finetune\n",
      "[NeMo W 2023-03-07 08:35:23 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:240: LightningDeprecationWarning: `ModelCheckpoint(every_n_val_epochs)` is deprecated in v1.4 and will be removed in v1.6. Please use `every_n_epochs` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo I 2023-03-07 08:35:26 tokenizer_utils:100] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /root/.cache/huggingface/nemo_nlp_tmp/ae0d012864bdb2474ba67537c3f5e0fa/tokenizer.vocab_file, special_tokens_dict: {}, and use_fast: False\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2023-03-07 08:35:27 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2023-03-07 08:35:27 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2023-03-07 08:35:37 finetune:118] Model restored from '/results/text_classification/train/checkpoints/trained-model.tlt'\n",
      "[NeMo I 2023-03-07 08:35:37 text_classification_dataset:120] Read 67349 examples from /data/sst2/train.tsv.\n",
      "[NeMo I 2023-03-07 08:35:37 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-03-07 08:35:37 text_classification_dataset:239] example 0: ['fails', 'to', 'match', 'the', 'freshness', 'of', 'the', 'actress-producer', 'and', 'writer']\n",
      "[NeMo I 2023-03-07 08:35:37 text_classification_dataset:240] subtokens: [CLS] fails to match the fresh ##ness of the actress - producer and writer [SEP]\n",
      "[NeMo I 2023-03-07 08:35:37 text_classification_dataset:241] input_ids: 101 11896 2000 2674 1996 4840 2791 1997 1996 3883 1011 3135 1998 3213 102\n",
      "[NeMo I 2023-03-07 08:35:37 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2023-03-07 08:35:37 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2023-03-07 08:35:37 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2023-03-07 08:35:37 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-03-07 08:35:37 text_classification_dataset:239] example 1: ['it', \"'s\", 'unnerving', 'suspense']\n",
      "[NeMo I 2023-03-07 08:35:37 text_classification_dataset:240] subtokens: [CLS] it ' s un ##ner ##ving suspense [SEP]\n",
      "[NeMo I 2023-03-07 08:35:37 text_classification_dataset:241] input_ids: 101 2009 1005 1055 4895 3678 6455 23873 102\n",
      "[NeMo I 2023-03-07 08:35:37 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2023-03-07 08:35:37 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2023-03-07 08:35:37 text_classification_dataset:244] label: 1\n",
      "[NeMo I 2023-03-07 08:36:13 data_preprocessing:358] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-03-07 08:36:13 data_preprocessing:360] Min: 3 |                  Max: 66 |                  Mean: 13.319262349849293 |                  Median: 10.0\n",
      "[NeMo I 2023-03-07 08:36:13 data_preprocessing:366] 75 percentile: 18.00\n",
      "[NeMo I 2023-03-07 08:36:13 data_preprocessing:367] 99 percentile: 44.00\n",
      "[NeMo I 2023-03-07 08:36:15 text_classification_dataset:120] Read 872 examples from /data/sst2/dev.tsv.\n",
      "[NeMo I 2023-03-07 08:36:15 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-03-07 08:36:15 text_classification_dataset:239] example 0: ['it', \"'s\", 'a', 'charming', 'and', 'often', 'affecting', 'journey', '.']\n",
      "[NeMo I 2023-03-07 08:36:15 text_classification_dataset:240] subtokens: [CLS] it ' s a charming and often affecting journey . [SEP]\n",
      "[NeMo I 2023-03-07 08:36:15 text_classification_dataset:241] input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102\n",
      "[NeMo I 2023-03-07 08:36:15 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2023-03-07 08:36:15 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2023-03-07 08:36:15 text_classification_dataset:244] label: 1\n",
      "[NeMo I 2023-03-07 08:36:15 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-03-07 08:36:15 text_classification_dataset:239] example 1: ['unflinchingly', 'bleak', 'and', 'desperate']\n",
      "[NeMo I 2023-03-07 08:36:15 text_classification_dataset:240] subtokens: [CLS] un ##fl ##in ##ching ##ly bleak and desperate [SEP]\n",
      "[NeMo I 2023-03-07 08:36:15 text_classification_dataset:241] input_ids: 101 4895 10258 2378 8450 2135 21657 1998 7143 102\n",
      "[NeMo I 2023-03-07 08:36:15 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2023-03-07 08:36:15 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2023-03-07 08:36:15 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2023-03-07 08:36:16 data_preprocessing:358] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-03-07 08:36:16 data_preprocessing:360] Min: 4 |                  Max: 55 |                  Mean: 25.163990825688074 |                  Median: 24.5\n",
      "[NeMo I 2023-03-07 08:36:16 data_preprocessing:366] 75 percentile: 32.00\n",
      "[NeMo I 2023-03-07 08:36:16 data_preprocessing:367] 99 percentile: 51.29\n",
      "[NeMo W 2023-03-07 08:36:16 modelPT:436] Trainer wasn't specified in model constructor. Make sure that you really wanted it.\n",
      "[NeMo I 2023-03-07 08:36:16 modelPT:544] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.9998]\n",
      "        eps: 1e-08\n",
      "        lr: 0.001\n",
      "        weight_decay: 0.001\n",
      "    )\n",
      "[NeMo I 2023-03-07 08:36:16 lr_scheduler:496] Scheduler not initialized as no `sched` config supplied to setup_optimizer()\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for 1 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All DDP processes registered. Starting ddp with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo W 2023-03-07 08:36:16 modelPT:197] You tried to register an artifact under config key=language_model.config_file but an artifact for it has already been registered.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2023-03-07 08:36:16 modelPT:415] No optimizer config provided, therefore no optimizer was created\n",
      "\n",
      "    | Name                                                   | Type                 | Params\n",
      "--------------------------------------------------------------------------------------------------\n",
      "0   | bert_model                                             | BertEncoder          | 109 M \n",
      "1   | bert_model.embeddings                                  | BertEmbeddings       | 23.8 M\n",
      "2   | bert_model.embeddings.word_embeddings                  | Embedding            | 23.4 M\n",
      "3   | bert_model.embeddings.position_embeddings              | Embedding            | 393 K \n",
      "4   | bert_model.embeddings.token_type_embeddings            | Embedding            | 1.5 K \n",
      "5   | bert_model.embeddings.LayerNorm                        | LayerNorm            | 1.5 K \n",
      "6   | bert_model.embeddings.dropout                          | Dropout              | 0     \n",
      "7   | bert_model.encoder                                     | BertEncoder          | 85.1 M\n",
      "8   | bert_model.encoder.layer                               | ModuleList           | 85.1 M\n",
      "9   | bert_model.encoder.layer.0                             | BertLayer            | 7.1 M \n",
      "10  | bert_model.encoder.layer.0.attention                   | BertAttention        | 2.4 M \n",
      "11  | bert_model.encoder.layer.0.attention.self              | BertSelfAttention    | 1.8 M \n",
      "12  | bert_model.encoder.layer.0.attention.self.query        | Linear               | 590 K \n",
      "13  | bert_model.encoder.layer.0.attention.self.key          | Linear               | 590 K \n",
      "14  | bert_model.encoder.layer.0.attention.self.value        | Linear               | 590 K \n",
      "15  | bert_model.encoder.layer.0.attention.self.dropout      | Dropout              | 0     \n",
      "16  | bert_model.encoder.layer.0.attention.output            | BertSelfOutput       | 592 K \n",
      "17  | bert_model.encoder.layer.0.attention.output.dense      | Linear               | 590 K \n",
      "18  | bert_model.encoder.layer.0.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "19  | bert_model.encoder.layer.0.attention.output.dropout    | Dropout              | 0     \n",
      "20  | bert_model.encoder.layer.0.intermediate                | BertIntermediate     | 2.4 M \n",
      "21  | bert_model.encoder.layer.0.intermediate.dense          | Linear               | 2.4 M \n",
      "22  | bert_model.encoder.layer.0.output                      | BertOutput           | 2.4 M \n",
      "23  | bert_model.encoder.layer.0.output.dense                | Linear               | 2.4 M \n",
      "24  | bert_model.encoder.layer.0.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "25  | bert_model.encoder.layer.0.output.dropout              | Dropout              | 0     \n",
      "26  | bert_model.encoder.layer.1                             | BertLayer            | 7.1 M \n",
      "27  | bert_model.encoder.layer.1.attention                   | BertAttention        | 2.4 M \n",
      "28  | bert_model.encoder.layer.1.attention.self              | BertSelfAttention    | 1.8 M \n",
      "29  | bert_model.encoder.layer.1.attention.self.query        | Linear               | 590 K \n",
      "30  | bert_model.encoder.layer.1.attention.self.key          | Linear               | 590 K \n",
      "31  | bert_model.encoder.layer.1.attention.self.value        | Linear               | 590 K \n",
      "32  | bert_model.encoder.layer.1.attention.self.dropout      | Dropout              | 0     \n",
      "33  | bert_model.encoder.layer.1.attention.output            | BertSelfOutput       | 592 K \n",
      "34  | bert_model.encoder.layer.1.attention.output.dense      | Linear               | 590 K \n",
      "35  | bert_model.encoder.layer.1.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "36  | bert_model.encoder.layer.1.attention.output.dropout    | Dropout              | 0     \n",
      "37  | bert_model.encoder.layer.1.intermediate                | BertIntermediate     | 2.4 M \n",
      "38  | bert_model.encoder.layer.1.intermediate.dense          | Linear               | 2.4 M \n",
      "39  | bert_model.encoder.layer.1.output                      | BertOutput           | 2.4 M \n",
      "40  | bert_model.encoder.layer.1.output.dense                | Linear               | 2.4 M \n",
      "41  | bert_model.encoder.layer.1.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "42  | bert_model.encoder.layer.1.output.dropout              | Dropout              | 0     \n",
      "43  | bert_model.encoder.layer.2                             | BertLayer            | 7.1 M \n",
      "44  | bert_model.encoder.layer.2.attention                   | BertAttention        | 2.4 M \n",
      "45  | bert_model.encoder.layer.2.attention.self              | BertSelfAttention    | 1.8 M \n",
      "46  | bert_model.encoder.layer.2.attention.self.query        | Linear               | 590 K \n",
      "47  | bert_model.encoder.layer.2.attention.self.key          | Linear               | 590 K \n",
      "48  | bert_model.encoder.layer.2.attention.self.value        | Linear               | 590 K \n",
      "49  | bert_model.encoder.layer.2.attention.self.dropout      | Dropout              | 0     \n",
      "50  | bert_model.encoder.layer.2.attention.output            | BertSelfOutput       | 592 K \n",
      "51  | bert_model.encoder.layer.2.attention.output.dense      | Linear               | 590 K \n",
      "52  | bert_model.encoder.layer.2.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "53  | bert_model.encoder.layer.2.attention.output.dropout    | Dropout              | 0     \n",
      "54  | bert_model.encoder.layer.2.intermediate                | BertIntermediate     | 2.4 M \n",
      "55  | bert_model.encoder.layer.2.intermediate.dense          | Linear               | 2.4 M \n",
      "56  | bert_model.encoder.layer.2.output                      | BertOutput           | 2.4 M \n",
      "57  | bert_model.encoder.layer.2.output.dense                | Linear               | 2.4 M \n",
      "58  | bert_model.encoder.layer.2.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "59  | bert_model.encoder.layer.2.output.dropout              | Dropout              | 0     \n",
      "60  | bert_model.encoder.layer.3                             | BertLayer            | 7.1 M \n",
      "61  | bert_model.encoder.layer.3.attention                   | BertAttention        | 2.4 M \n",
      "62  | bert_model.encoder.layer.3.attention.self              | BertSelfAttention    | 1.8 M \n",
      "63  | bert_model.encoder.layer.3.attention.self.query        | Linear               | 590 K \n",
      "64  | bert_model.encoder.layer.3.attention.self.key          | Linear               | 590 K \n",
      "65  | bert_model.encoder.layer.3.attention.self.value        | Linear               | 590 K \n",
      "66  | bert_model.encoder.layer.3.attention.self.dropout      | Dropout              | 0     \n",
      "67  | bert_model.encoder.layer.3.attention.output            | BertSelfOutput       | 592 K \n",
      "68  | bert_model.encoder.layer.3.attention.output.dense      | Linear               | 590 K \n",
      "69  | bert_model.encoder.layer.3.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "70  | bert_model.encoder.layer.3.attention.output.dropout    | Dropout              | 0     \n",
      "71  | bert_model.encoder.layer.3.intermediate                | BertIntermediate     | 2.4 M \n",
      "72  | bert_model.encoder.layer.3.intermediate.dense          | Linear               | 2.4 M \n",
      "73  | bert_model.encoder.layer.3.output                      | BertOutput           | 2.4 M \n",
      "74  | bert_model.encoder.layer.3.output.dense                | Linear               | 2.4 M \n",
      "75  | bert_model.encoder.layer.3.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "76  | bert_model.encoder.layer.3.output.dropout              | Dropout              | 0     \n",
      "77  | bert_model.encoder.layer.4                             | BertLayer            | 7.1 M \n",
      "78  | bert_model.encoder.layer.4.attention                   | BertAttention        | 2.4 M \n",
      "79  | bert_model.encoder.layer.4.attention.self              | BertSelfAttention    | 1.8 M \n",
      "80  | bert_model.encoder.layer.4.attention.self.query        | Linear               | 590 K \n",
      "81  | bert_model.encoder.layer.4.attention.self.key          | Linear               | 590 K \n",
      "82  | bert_model.encoder.layer.4.attention.self.value        | Linear               | 590 K \n",
      "83  | bert_model.encoder.layer.4.attention.self.dropout      | Dropout              | 0     \n",
      "84  | bert_model.encoder.layer.4.attention.output            | BertSelfOutput       | 592 K \n",
      "85  | bert_model.encoder.layer.4.attention.output.dense      | Linear               | 590 K \n",
      "86  | bert_model.encoder.layer.4.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "87  | bert_model.encoder.layer.4.attention.output.dropout    | Dropout              | 0     \n",
      "88  | bert_model.encoder.layer.4.intermediate                | BertIntermediate     | 2.4 M \n",
      "89  | bert_model.encoder.layer.4.intermediate.dense          | Linear               | 2.4 M \n",
      "90  | bert_model.encoder.layer.4.output                      | BertOutput           | 2.4 M \n",
      "91  | bert_model.encoder.layer.4.output.dense                | Linear               | 2.4 M \n",
      "92  | bert_model.encoder.layer.4.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "93  | bert_model.encoder.layer.4.output.dropout              | Dropout              | 0     \n",
      "94  | bert_model.encoder.layer.5                             | BertLayer            | 7.1 M \n",
      "95  | bert_model.encoder.layer.5.attention                   | BertAttention        | 2.4 M \n",
      "96  | bert_model.encoder.layer.5.attention.self              | BertSelfAttention    | 1.8 M \n",
      "97  | bert_model.encoder.layer.5.attention.self.query        | Linear               | 590 K \n",
      "98  | bert_model.encoder.layer.5.attention.self.key          | Linear               | 590 K \n",
      "99  | bert_model.encoder.layer.5.attention.self.value        | Linear               | 590 K \n",
      "100 | bert_model.encoder.layer.5.attention.self.dropout      | Dropout              | 0     \n",
      "101 | bert_model.encoder.layer.5.attention.output            | BertSelfOutput       | 592 K \n",
      "102 | bert_model.encoder.layer.5.attention.output.dense      | Linear               | 590 K \n",
      "103 | bert_model.encoder.layer.5.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "104 | bert_model.encoder.layer.5.attention.output.dropout    | Dropout              | 0     \n",
      "105 | bert_model.encoder.layer.5.intermediate                | BertIntermediate     | 2.4 M \n",
      "106 | bert_model.encoder.layer.5.intermediate.dense          | Linear               | 2.4 M \n",
      "107 | bert_model.encoder.layer.5.output                      | BertOutput           | 2.4 M \n",
      "108 | bert_model.encoder.layer.5.output.dense                | Linear               | 2.4 M \n",
      "109 | bert_model.encoder.layer.5.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "110 | bert_model.encoder.layer.5.output.dropout              | Dropout              | 0     \n",
      "111 | bert_model.encoder.layer.6                             | BertLayer            | 7.1 M \n",
      "112 | bert_model.encoder.layer.6.attention                   | BertAttention        | 2.4 M \n",
      "113 | bert_model.encoder.layer.6.attention.self              | BertSelfAttention    | 1.8 M \n",
      "114 | bert_model.encoder.layer.6.attention.self.query        | Linear               | 590 K \n",
      "115 | bert_model.encoder.layer.6.attention.self.key          | Linear               | 590 K \n",
      "116 | bert_model.encoder.layer.6.attention.self.value        | Linear               | 590 K \n",
      "117 | bert_model.encoder.layer.6.attention.self.dropout      | Dropout              | 0     \n",
      "118 | bert_model.encoder.layer.6.attention.output            | BertSelfOutput       | 592 K \n",
      "119 | bert_model.encoder.layer.6.attention.output.dense      | Linear               | 590 K \n",
      "120 | bert_model.encoder.layer.6.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "121 | bert_model.encoder.layer.6.attention.output.dropout    | Dropout              | 0     \n",
      "122 | bert_model.encoder.layer.6.intermediate                | BertIntermediate     | 2.4 M \n",
      "123 | bert_model.encoder.layer.6.intermediate.dense          | Linear               | 2.4 M \n",
      "124 | bert_model.encoder.layer.6.output                      | BertOutput           | 2.4 M \n",
      "125 | bert_model.encoder.layer.6.output.dense                | Linear               | 2.4 M \n",
      "126 | bert_model.encoder.layer.6.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "127 | bert_model.encoder.layer.6.output.dropout              | Dropout              | 0     \n",
      "128 | bert_model.encoder.layer.7                             | BertLayer            | 7.1 M \n",
      "129 | bert_model.encoder.layer.7.attention                   | BertAttention        | 2.4 M \n",
      "130 | bert_model.encoder.layer.7.attention.self              | BertSelfAttention    | 1.8 M \n",
      "131 | bert_model.encoder.layer.7.attention.self.query        | Linear               | 590 K \n",
      "132 | bert_model.encoder.layer.7.attention.self.key          | Linear               | 590 K \n",
      "133 | bert_model.encoder.layer.7.attention.self.value        | Linear               | 590 K \n",
      "134 | bert_model.encoder.layer.7.attention.self.dropout      | Dropout              | 0     \n",
      "135 | bert_model.encoder.layer.7.attention.output            | BertSelfOutput       | 592 K \n",
      "136 | bert_model.encoder.layer.7.attention.output.dense      | Linear               | 590 K \n",
      "137 | bert_model.encoder.layer.7.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "138 | bert_model.encoder.layer.7.attention.output.dropout    | Dropout              | 0     \n",
      "139 | bert_model.encoder.layer.7.intermediate                | BertIntermediate     | 2.4 M \n",
      "140 | bert_model.encoder.layer.7.intermediate.dense          | Linear               | 2.4 M \n",
      "141 | bert_model.encoder.layer.7.output                      | BertOutput           | 2.4 M \n",
      "142 | bert_model.encoder.layer.7.output.dense                | Linear               | 2.4 M \n",
      "143 | bert_model.encoder.layer.7.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "144 | bert_model.encoder.layer.7.output.dropout              | Dropout              | 0     \n",
      "145 | bert_model.encoder.layer.8                             | BertLayer            | 7.1 M \n",
      "146 | bert_model.encoder.layer.8.attention                   | BertAttention        | 2.4 M \n",
      "147 | bert_model.encoder.layer.8.attention.self              | BertSelfAttention    | 1.8 M \n",
      "148 | bert_model.encoder.layer.8.attention.self.query        | Linear               | 590 K \n",
      "149 | bert_model.encoder.layer.8.attention.self.key          | Linear               | 590 K \n",
      "150 | bert_model.encoder.layer.8.attention.self.value        | Linear               | 590 K \n",
      "151 | bert_model.encoder.layer.8.attention.self.dropout      | Dropout              | 0     \n",
      "152 | bert_model.encoder.layer.8.attention.output            | BertSelfOutput       | 592 K \n",
      "153 | bert_model.encoder.layer.8.attention.output.dense      | Linear               | 590 K \n",
      "154 | bert_model.encoder.layer.8.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "155 | bert_model.encoder.layer.8.attention.output.dropout    | Dropout              | 0     \n",
      "156 | bert_model.encoder.layer.8.intermediate                | BertIntermediate     | 2.4 M \n",
      "157 | bert_model.encoder.layer.8.intermediate.dense          | Linear               | 2.4 M \n",
      "158 | bert_model.encoder.layer.8.output                      | BertOutput           | 2.4 M \n",
      "159 | bert_model.encoder.layer.8.output.dense                | Linear               | 2.4 M \n",
      "160 | bert_model.encoder.layer.8.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "161 | bert_model.encoder.layer.8.output.dropout              | Dropout              | 0     \n",
      "162 | bert_model.encoder.layer.9                             | BertLayer            | 7.1 M \n",
      "163 | bert_model.encoder.layer.9.attention                   | BertAttention        | 2.4 M \n",
      "164 | bert_model.encoder.layer.9.attention.self              | BertSelfAttention    | 1.8 M \n",
      "165 | bert_model.encoder.layer.9.attention.self.query        | Linear               | 590 K \n",
      "166 | bert_model.encoder.layer.9.attention.self.key          | Linear               | 590 K \n",
      "167 | bert_model.encoder.layer.9.attention.self.value        | Linear               | 590 K \n",
      "168 | bert_model.encoder.layer.9.attention.self.dropout      | Dropout              | 0     \n",
      "169 | bert_model.encoder.layer.9.attention.output            | BertSelfOutput       | 592 K \n",
      "170 | bert_model.encoder.layer.9.attention.output.dense      | Linear               | 590 K \n",
      "171 | bert_model.encoder.layer.9.attention.output.LayerNorm  | LayerNorm            | 1.5 K \n",
      "172 | bert_model.encoder.layer.9.attention.output.dropout    | Dropout              | 0     \n",
      "173 | bert_model.encoder.layer.9.intermediate                | BertIntermediate     | 2.4 M \n",
      "174 | bert_model.encoder.layer.9.intermediate.dense          | Linear               | 2.4 M \n",
      "175 | bert_model.encoder.layer.9.output                      | BertOutput           | 2.4 M \n",
      "176 | bert_model.encoder.layer.9.output.dense                | Linear               | 2.4 M \n",
      "177 | bert_model.encoder.layer.9.output.LayerNorm            | LayerNorm            | 1.5 K \n",
      "178 | bert_model.encoder.layer.9.output.dropout              | Dropout              | 0     \n",
      "179 | bert_model.encoder.layer.10                            | BertLayer            | 7.1 M \n",
      "180 | bert_model.encoder.layer.10.attention                  | BertAttention        | 2.4 M \n",
      "181 | bert_model.encoder.layer.10.attention.self             | BertSelfAttention    | 1.8 M \n",
      "182 | bert_model.encoder.layer.10.attention.self.query       | Linear               | 590 K \n",
      "183 | bert_model.encoder.layer.10.attention.self.key         | Linear               | 590 K \n",
      "184 | bert_model.encoder.layer.10.attention.self.value       | Linear               | 590 K \n",
      "185 | bert_model.encoder.layer.10.attention.self.dropout     | Dropout              | 0     \n",
      "186 | bert_model.encoder.layer.10.attention.output           | BertSelfOutput       | 592 K \n",
      "187 | bert_model.encoder.layer.10.attention.output.dense     | Linear               | 590 K \n",
      "188 | bert_model.encoder.layer.10.attention.output.LayerNorm | LayerNorm            | 1.5 K \n",
      "189 | bert_model.encoder.layer.10.attention.output.dropout   | Dropout              | 0     \n",
      "190 | bert_model.encoder.layer.10.intermediate               | BertIntermediate     | 2.4 M \n",
      "191 | bert_model.encoder.layer.10.intermediate.dense         | Linear               | 2.4 M \n",
      "192 | bert_model.encoder.layer.10.output                     | BertOutput           | 2.4 M \n",
      "193 | bert_model.encoder.layer.10.output.dense               | Linear               | 2.4 M \n",
      "194 | bert_model.encoder.layer.10.output.LayerNorm           | LayerNorm            | 1.5 K \n",
      "195 | bert_model.encoder.layer.10.output.dropout             | Dropout              | 0     \n",
      "196 | bert_model.encoder.layer.11                            | BertLayer            | 7.1 M \n",
      "197 | bert_model.encoder.layer.11.attention                  | BertAttention        | 2.4 M \n",
      "198 | bert_model.encoder.layer.11.attention.self             | BertSelfAttention    | 1.8 M \n",
      "199 | bert_model.encoder.layer.11.attention.self.query       | Linear               | 590 K \n",
      "200 | bert_model.encoder.layer.11.attention.self.key         | Linear               | 590 K \n",
      "201 | bert_model.encoder.layer.11.attention.self.value       | Linear               | 590 K \n",
      "202 | bert_model.encoder.layer.11.attention.self.dropout     | Dropout              | 0     \n",
      "203 | bert_model.encoder.layer.11.attention.output           | BertSelfOutput       | 592 K \n",
      "204 | bert_model.encoder.layer.11.attention.output.dense     | Linear               | 590 K \n",
      "205 | bert_model.encoder.layer.11.attention.output.LayerNorm | LayerNorm            | 1.5 K \n",
      "206 | bert_model.encoder.layer.11.attention.output.dropout   | Dropout              | 0     \n",
      "207 | bert_model.encoder.layer.11.intermediate               | BertIntermediate     | 2.4 M \n",
      "208 | bert_model.encoder.layer.11.intermediate.dense         | Linear               | 2.4 M \n",
      "209 | bert_model.encoder.layer.11.output                     | BertOutput           | 2.4 M \n",
      "210 | bert_model.encoder.layer.11.output.dense               | Linear               | 2.4 M \n",
      "211 | bert_model.encoder.layer.11.output.LayerNorm           | LayerNorm            | 1.5 K \n",
      "212 | bert_model.encoder.layer.11.output.dropout             | Dropout              | 0     \n",
      "213 | bert_model.pooler                                      | BertPooler           | 590 K \n",
      "214 | bert_model.pooler.dense                                | Linear               | 590 K \n",
      "215 | bert_model.pooler.activation                           | Tanh                 | 0     \n",
      "216 | classifier                                             | SequenceClassifier   | 592 K \n",
      "217 | classifier.dropout                                     | Dropout              | 0     \n",
      "218 | classifier.mlp                                         | MultiLayerPerceptron | 592 K \n",
      "219 | classifier.mlp.layer0                                  | Linear               | 590 K \n",
      "220 | classifier.mlp.layer2                                  | Linear               | 1.5 K \n",
      "221 | loss                                                   | CrossEntropyLoss     | 0     \n",
      "222 | classification_report                                  | ClassificationReport | 0     \n",
      "--------------------------------------------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.297   Total estimated model params size (MB)\n",
      "Validation sanity check: 100%|████████████████████| 2/2 [00:01<00:00,  1.22it/s][NeMo I 2023-03-07 08:36:17 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             92.73      83.61      87.93         61\n",
      "    label_id: 1                                             86.30      94.03      90.00         67\n",
      "    -------------------\n",
      "    micro avg                                               89.06      89.06      89.06        128\n",
      "    macro avg                                               89.51      88.82      88.97        128\n",
      "    weighted avg                                            89.36      89.06      89.01        128\n",
      "    \n",
      "Epoch 0:  99%|███████▉| 1053/1067 [07:51<00:06,  2.23it/s, loss=0.689, lr=0.001]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  99%|███████▉| 1055/1067 [07:51<00:05,  2.24it/s, loss=0.689, lr=0.001]\u001b[A\n",
      "Validating:  14%|████▌                           | 2/14 [00:00<00:03,  3.88it/s]\u001b[A\n",
      "Epoch 0:  99%|███████▉| 1057/1067 [07:52<00:04,  2.24it/s, loss=0.689, lr=0.001]\u001b[A\n",
      "Validating:  29%|█████████▏                      | 4/14 [00:00<00:02,  4.54it/s]\u001b[A\n",
      "Epoch 0:  99%|███████▉| 1059/1067 [07:52<00:03,  2.24it/s, loss=0.689, lr=0.001]\u001b[A\n",
      "Validating:  43%|█████████████▋                  | 6/14 [00:01<00:01,  5.05it/s]\u001b[A\n",
      "Epoch 0:  99%|███████▉| 1061/1067 [07:53<00:02,  2.25it/s, loss=0.689, lr=0.001]\u001b[A\n",
      "Validating:  57%|██████████████████▎             | 8/14 [00:01<00:01,  5.37it/s]\u001b[A\n",
      "Epoch 0: 100%|███████▉| 1063/1067 [07:53<00:01,  2.25it/s, loss=0.689, lr=0.001]\u001b[A\n",
      "Validating:  71%|██████████████████████▏        | 10/14 [00:01<00:00,  5.54it/s]\u001b[A\n",
      "Epoch 0: 100%|███████▉| 1065/1067 [07:53<00:00,  2.25it/s, loss=0.689, lr=0.001]\u001b[A\n",
      "Validating:  86%|██████████████████████████▌    | 12/14 [00:02<00:00,  5.64it/s]\u001b[A\n",
      "Epoch 0: 100%|████████| 1067/1067 [07:54<00:00,  2.25it/s, loss=0.689, lr=0.001]\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 14/14 [00:02<00:00,  6.16it/s]\u001b[A[NeMo I 2023-03-07 08:44:11 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                              0.00       0.00       0.00        428\n",
      "    label_id: 1                                             50.92     100.00      67.48        444\n",
      "    -------------------\n",
      "    micro avg                                               50.92      50.92      50.92        872\n",
      "    macro avg                                               25.46      50.00      33.74        872\n",
      "    weighted avg                                            25.93      50.92      34.36        872\n",
      "    \n",
      "Epoch 0: 100%|█| 1067/1067 [07:54<00:00,  2.25it/s, loss=0.689, lr=0.001, val_lo\n",
      "                                                                                \u001b[AEpoch 0, global step 1052: val_loss reached 0.69513 (best 0.69513), saving model to \"/results/text_classification/finetune/checkpoints/finetuned-model--val_loss=0.70-epoch=0.ckpt\" as top 3\n",
      "Epoch 0: 100%|█| 1067/1067 [08:18<00:00,  2.14it/s, loss=0.689, lr=0.001, val_lo\n",
      "[NeMo I 2023-03-07 08:44:55 finetune:131] Experiment logs saved to '/results/text_classification/finetune'\n",
      "[NeMo I 2023-03-07 08:44:55 finetune:132] Fine-tuned model saved to '/results/text_classification/finetune/checkpoints/finetuned-model.tlt'\n",
      "\u001b[0m\u001b[0m2023-03-07 08:44:58,186 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# 3. For BERT finetuning on SST-2:\n",
    "!tao text_classification finetune \\\n",
    "    -e $SPECS_DIR/finetune.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/finetune \\\n",
    "    finetuning_ds.file_path=$DATA_DIR/sst2/train.tsv \\\n",
    "    validation_ds.file_path=$DATA_DIR/sst2/dev.tsv \\\n",
    "    trainer.max_epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will generate a fine-tuned model `finetuned-model.tlt` at `$RESULTS_DIR/finetune/checkpoints`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='evaluation'></a>\n",
    "### Evaluation\n",
    "The evaluation spec .yaml is as simple as:\n",
    "\n",
    "```\n",
    "test_ds:\n",
    "  file: ??? # e.g. $DATA_DIR/test.tsv\n",
    "  batch_size: 32\n",
    "  shuffle: false\n",
    "  num_samples: 500\n",
    "```\n",
    "\n",
    "Below, we use `tao text_classification evaluate` and override the test data configuration by specifying `test_ds.file_path`. Other arguments follow the same pattern as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 08:44:59,774 [INFO] root: Registry: ['nvcr.io']\n",
      "2023-03-07 08:44:59,913 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2023-03-07 08:44:59,923 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2023-03-07 08:45:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:05 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:05 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2023-03-07 08:45:05 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2023-03-07 08:45:05 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2023-03-07 08:45:06 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:45:06 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:06 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:11 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:11 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:11 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:11 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2023-03-07 08:45:12 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2023-03-07 08:45:12 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:45:12 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:45:12 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:12 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:13 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/nlp/text_classification/scripts/evaluate.py:101: UserWarning: \n",
      "    'evaluate.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "[NeMo I 2023-03-07 08:45:13 tlt_logging:20] Experiment configuration:\n",
      "    restore_from: /results/text_classification/train/checkpoints/trained-model.tlt\n",
      "    exp_manager:\n",
      "      explicit_log_dir: /results/text_classification/evaluate\n",
      "      exp_dir: null\n",
      "      name: null\n",
      "      version: null\n",
      "      use_datetime_version: true\n",
      "      resume_if_exists: false\n",
      "      resume_past_end: false\n",
      "      resume_ignore_no_checkpoint: false\n",
      "      create_tensorboard_logger: false\n",
      "      summary_writer_kwargs: null\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs: null\n",
      "      create_checkpoint_callback: false\n",
      "      checkpoint_callback_params:\n",
      "        filepath: null\n",
      "        dirpath: null\n",
      "        filename: null\n",
      "        monitor: val_loss\n",
      "        verbose: true\n",
      "        save_last: true\n",
      "        save_top_k: 3\n",
      "        save_weights_only: false\n",
      "        mode: min\n",
      "        period: null\n",
      "        every_n_val_epochs: 1\n",
      "        prefix: null\n",
      "        postfix: .nemo\n",
      "        save_best_model: false\n",
      "        always_save_nemo: false\n",
      "      files_to_copy: null\n",
      "    trainer:\n",
      "      logger: false\n",
      "      checkpoint_callback: false\n",
      "      callbacks: null\n",
      "      default_root_dir: null\n",
      "      gradient_clip_val: 0.0\n",
      "      process_position: 0\n",
      "      num_nodes: 1\n",
      "      num_processes: 1\n",
      "      gpus: 1\n",
      "      auto_select_gpus: false\n",
      "      tpu_cores: null\n",
      "      log_gpu_memory: null\n",
      "      progress_bar_refresh_rate: 1\n",
      "      overfit_batches: 0.0\n",
      "      track_grad_norm: -1\n",
      "      check_val_every_n_epoch: 1\n",
      "      fast_dev_run: false\n",
      "      accumulate_grad_batches: 1\n",
      "      max_epochs: 1000\n",
      "      min_epochs: 1\n",
      "      max_steps: null\n",
      "      min_steps: null\n",
      "      limit_train_batches: 1.0\n",
      "      limit_val_batches: 1.0\n",
      "      limit_test_batches: 1.0\n",
      "      val_check_interval: 1.0\n",
      "      flush_logs_every_n_steps: 100\n",
      "      log_every_n_steps: 50\n",
      "      accelerator: ddp\n",
      "      sync_batchnorm: false\n",
      "      precision: 32\n",
      "      weights_summary: full\n",
      "      weights_save_path: null\n",
      "      num_sanity_val_steps: 2\n",
      "      truncated_bptt_steps: null\n",
      "      resume_from_checkpoint: null\n",
      "      profiler: null\n",
      "      benchmark: false\n",
      "      deterministic: false\n",
      "      reload_dataloaders_every_epoch: false\n",
      "      auto_lr_find: false\n",
      "      replace_sampler_ddp: true\n",
      "      terminate_on_nan: false\n",
      "      auto_scale_batch_size: false\n",
      "      prepare_data_per_node: true\n",
      "      amp_backend: native\n",
      "      amp_level: O2\n",
      "      plugins: null\n",
      "      move_metrics_to_cpu: false\n",
      "      multiple_trainloader_mode: max_size_cycle\n",
      "      limit_predict_batches: 1.0\n",
      "      stochastic_weight_avg: false\n",
      "      gradient_clip_algorithm: norm\n",
      "      max_time: null\n",
      "      reload_dataloaders_every_n_epochs: 0\n",
      "      ipus: null\n",
      "      devices: null\n",
      "    test_ds:\n",
      "      file_path: /data/SST-2/dev.tsv\n",
      "      batch_size: 32\n",
      "      shuffle: false\n",
      "      num_samples: -1\n",
      "      num_workers: 3\n",
      "      drop_last: false\n",
      "      pin_memory: false\n",
      "    encryption_key: '*******'\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[NeMo I 2023-03-07 08:45:13 exp_manager:220] Experiments will be logged at /results/text_classification/evaluate\n",
      "[NeMo I 2023-03-07 08:45:16 tokenizer_utils:100] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /root/.cache/huggingface/nemo_nlp_tmp/ae0d012864bdb2474ba67537c3f5e0fa/tokenizer.vocab_file, special_tokens_dict: {}, and use_fast: False\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2023-03-07 08:45:16 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2023-03-07 08:45:16 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2023-03-07 08:45:27 text_classification_dataset:120] Read 873 examples from /data/SST-2/dev.tsv.\n",
      "[NeMo I 2023-03-07 08:45:27 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-03-07 08:45:27 text_classification_dataset:239] example 0: ['it', \"'s\", 'a', 'charming', 'and', 'often', 'affecting', 'journey', '.']\n",
      "[NeMo I 2023-03-07 08:45:27 text_classification_dataset:240] subtokens: [CLS] it ' s a charming and often affecting journey . [SEP]\n",
      "[NeMo I 2023-03-07 08:45:27 text_classification_dataset:241] input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102\n",
      "[NeMo I 2023-03-07 08:45:27 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2023-03-07 08:45:27 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2023-03-07 08:45:27 text_classification_dataset:244] label: 1\n",
      "[NeMo I 2023-03-07 08:45:27 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-03-07 08:45:27 text_classification_dataset:239] example 1: ['unflinchingly', 'bleak', 'and', 'desperate']\n",
      "[NeMo I 2023-03-07 08:45:27 text_classification_dataset:240] subtokens: [CLS] un ##fl ##in ##ching ##ly bleak and desperate [SEP]\n",
      "[NeMo I 2023-03-07 08:45:27 text_classification_dataset:241] input_ids: 101 4895 10258 2378 8450 2135 21657 1998 7143 102\n",
      "[NeMo I 2023-03-07 08:45:27 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2023-03-07 08:45:27 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2023-03-07 08:45:27 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2023-03-07 08:45:28 data_preprocessing:358] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-03-07 08:45:28 data_preprocessing:360] Min: 4 |                  Max: 55 |                  Mean: 25.163990825688074 |                  Median: 24.5\n",
      "[NeMo I 2023-03-07 08:45:28 data_preprocessing:366] 75 percentile: 32.00\n",
      "[NeMo I 2023-03-07 08:45:28 data_preprocessing:367] 99 percentile: 51.29\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for 1 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All DDP processes registered. Starting ddp with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Testing: 100%|██████████████████████████████████| 28/28 [00:03<00:00, 12.25it/s][NeMo I 2023-03-07 08:45:31 text_classification_model:165] test_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             91.83      89.25      90.52        428\n",
      "    label_id: 1                                             89.91      92.34      91.11        444\n",
      "    -------------------\n",
      "    micro avg                                               90.83      90.83      90.83        872\n",
      "    macro avg                                               90.87      90.80      90.82        872\n",
      "    weighted avg                                            90.85      90.83      90.82        872\n",
      "    \n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_f1': 90.82569885253906,\n",
      " 'test_loss': 0.2392188310623169,\n",
      " 'test_precision': 90.82569122314453,\n",
      " 'test_recall': 90.82569122314453}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████████████████████████████| 28/28 [00:03<00:00,  8.31it/s]\n",
      "[NeMo I 2023-03-07 08:45:31 evaluate:97] Experiment logs saved to '/results/text_classification/evaluate'\n",
      "\u001b[0m\u001b[0m2023-03-07 08:45:33,813 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# 4. For BERT evaluation on SST-2:\n",
    "!tao text_classification evaluate \\\n",
    "    -e $SPECS_DIR/evaluate.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/evaluate \\\n",
    "    test_ds.file_path=$DATA_DIR/SST-2/dev.tsv \\\n",
    "    test_ds.batch_size=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On evaluating the model you will get some results, and based on that you can either retrain the model for more epochs, or continue with the inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='inference'></a>\n",
    "### Inference\n",
    "Inference using a .tlt trained or fine-tuned model uses the `tao text_classification infer` command.  <br>\n",
    "The infer.yaml is also straightforward, which includes some \"simulated\" user input, here we start with a batch of four samples. \n",
    "\n",
    "- \"by the end of no such thing the audience , like beatrice , has a watchful affection for the monster .\"\n",
    "- \"director rob marshall went out gunning to make a great one .\"\n",
    "- \"uneasy mishmash of styles and genres .\"\n",
    "- \"I love exotic science fiction / fantasy movies but this one was very unpleasant to watch . Suggestions and images of child abuse , mutilated bodies (live or dead) , other gruesome scenes , plot holes , boring acting made this a regrettable experience , The basic idea of entering another person's mind is not even new to the movies or TV (An Outer Limits episode was better at exploring this idea) . i gave it 4 / 10 since some special effects were nice .\"\n",
    "\n",
    "We encourage you to try out custom inputs as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 08:45:34,779 [INFO] root: Registry: ['nvcr.io']\n",
      "2023-03-07 08:45:34,894 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2023-03-07 08:45:34,905 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2023-03-07 08:45:39 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:39 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:39 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:39 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2023-03-07 08:45:40 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2023-03-07 08:45:40 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2023-03-07 08:45:40 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:45:40 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:40 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:45 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:45 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:45 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:45 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2023-03-07 08:45:46 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2023-03-07 08:45:46 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:45:46 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:45:46 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:46 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:45:47 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/nlp/text_classification/scripts/infer.py:116: UserWarning: \n",
      "    'infer.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "[NeMo I 2023-03-07 08:45:47 tlt_logging:20] Experiment configuration:\n",
      "    restore_from: /results/text_classification/finetune/checkpoints/finetuned-model.tlt\n",
      "    exp_manager:\n",
      "      task_name: infer\n",
      "      explicit_log_dir: /results/text_classification/infer\n",
      "    input_batch:\n",
      "    - by the end of no such thing the audience , like beatrice , has a watchful affection\n",
      "      for the monster .\n",
      "    - director rob marshall went out gunning to make a great one .\n",
      "    - uneasy mishmash of styles and genres .\n",
      "    - I love exotic science fiction / fantasy movies but this one was very unpleasant\n",
      "      to watch . Suggestions and images of child abuse , mutilated bodies (live or dead)\n",
      "      , other gruesome scenes , plot holes , boring acting made this a regretable experience\n",
      "      , The basic idea of entering another person's mind is not even new to the movies\n",
      "      or TV (An Outer Limits episode was better at exploring this idea) . i gave it 4\n",
      "      / 10 since some special effects were nice .\n",
      "    encryption_key: '********'\n",
      "    \n",
      "[NeMo I 2023-03-07 08:45:50 tokenizer_utils:100] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /root/.cache/huggingface/nemo_nlp_tmp/ae0d012864bdb2474ba67537c3f5e0fa/tokenizer.vocab_file, special_tokens_dict: {}, and use_fast: False\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2023-03-07 08:45:50 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2023-03-07 08:45:50 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2023-03-07 08:46:02 infer:106] The prediction results of some sample queries with the trained model:\n",
      "[NeMo I 2023-03-07 08:46:02 infer:108] Query: by the end of no such thing the audience , like beatrice , has a watchful affection for the monster .\n",
      "[NeMo I 2023-03-07 08:46:02 infer:109] Predicted label: positive\n",
      "[NeMo I 2023-03-07 08:46:02 infer:108] Query: director rob marshall went out gunning to make a great one .\n",
      "[NeMo I 2023-03-07 08:46:02 infer:109] Predicted label: positive\n",
      "[NeMo I 2023-03-07 08:46:02 infer:108] Query: uneasy mishmash of styles and genres .\n",
      "[NeMo I 2023-03-07 08:46:02 infer:109] Predicted label: positive\n",
      "[NeMo I 2023-03-07 08:46:02 infer:108] Query: I love exotic science fiction / fantasy movies but this one was very unpleasant to watch . Suggestions and images of child abuse , mutilated bodies (live or dead) , other gruesome scenes , plot holes , boring acting made this a regretable experience , The basic idea of entering another person's mind is not even new to the movies or TV (An Outer Limits episode was better at exploring this idea) . i gave it 4 / 10 since some special effects were nice .\n",
      "[NeMo I 2023-03-07 08:46:02 infer:109] Predicted label: positive\n",
      "[NeMo I 2023-03-07 08:46:02 infer:112] Experiment logs saved to '/results/text_classification/infer'\n",
      "\u001b[0m\u001b[0m2023-03-07 08:46:04,433 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# 5. For BERT inference on user data:\n",
    "!tao text_classification infer \\\n",
    "    -e $SPECS_DIR/infer.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $RESULTS_DIR/finetune/checkpoints/finetuned-model.tlt \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='export-onnx'></a>\n",
    "### Export to ONNX\n",
    "\n",
    "[ONNX](https://onnx.ai/) is a popular open format for machine learning models. It enables interoperability between different frameworks, making the path to production much easier. \n",
    "\n",
    "TAO provides commands to export the .tlt model to the ONNX format in an .eonnx archive. The `export_format` configuration can be set to `ONNX` to achieve this.\n",
    "\n",
    "Sample usage of the `tao text_classification export` command is shown in the following code cell. The export.yaml file we use looks like:\n",
    "```\n",
    "# Name of the .tlt EFF archive to be loaded/model to be exported.\n",
    "restore_from: finetuned-model.tlt\n",
    "\n",
    "# Set export format: ONNX | RIVA\n",
    "export_format: ONNX\n",
    "\n",
    "# Output EFF archive containing ONNX.\n",
    "export_to: exported-model.eonnx\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 08:46:05,400 [INFO] root: Registry: ['nvcr.io']\n",
      "2023-03-07 08:46:05,518 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2023-03-07 08:46:05,529 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2023-03-07 08:46:10 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:46:10 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:46:10 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:46:10 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2023-03-07 08:46:10 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2023-03-07 08:46:11 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2023-03-07 08:46:11 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:46:11 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:46:11 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:46:16 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:46:16 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:46:16 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:46:16 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2023-03-07 08:46:17 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2023-03-07 08:46:17 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:46:17 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:46:17 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:46:17 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:46:18 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/nlp/text_classification/scripts/export.py:97: UserWarning: \n",
      "    'export.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "[NeMo I 2023-03-07 08:46:18 tlt_logging:20] Experiment configuration:\n",
      "    restore_from: /results/text_classification/finetune/checkpoints/finetuned-model.tlt\n",
      "    export_to: exported-model.riva\n",
      "    export_format: ONNX\n",
      "    exp_manager:\n",
      "      task_name: export\n",
      "      explicit_log_dir: /results/text_classification/export\n",
      "    encryption_key: '*******'\n",
      "    \n",
      "[NeMo I 2023-03-07 08:46:20 tokenizer_utils:100] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /root/.cache/huggingface/nemo_nlp_tmp/ae0d012864bdb2474ba67537c3f5e0fa/tokenizer.vocab_file, special_tokens_dict: {}, and use_fast: False\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2023-03-07 08:46:21 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2023-03-07 08:46:21 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2023-03-07 08:46:31 export:56] Model restored from '/results/text_classification/finetune/checkpoints/finetuned-model.tlt'\n",
      "[NeMo W 2023-03-07 08:46:32 export_utils:198] Swapped 0 modules\n",
      "[NeMo I 2023-03-07 08:47:00 export:79] Experiment logs saved to '/results/text_classification/export'\n",
      "[NeMo I 2023-03-07 08:47:00 export:80] Exported model to '/results/text_classification/export/exported-model.eonnx'\n",
      "\u001b[0m\u001b[0m2023-03-07 08:47:02,794 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# 6. For export to ONNX:\n",
    "!tao text_classification export \\\n",
    "    -e $SPECS_DIR/export.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $RESULTS_DIR/finetune/checkpoints/finetuned-model.tlt \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/export \\\n",
    "    export_format=ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command exports the model as `exported-model.eonnx` which is essentially an archive containing the .onnx model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using ONNX\n",
    "\n",
    "TAO provides the capability to use the exported .eonnx model for inference. \n",
    "\n",
    "The command `tao text_classification infer_onnx` is very similar to the inference command for .tlt models. Again, the input file includes some \"simulated\" user input, we start with a batch of four samples for demo purposes, and encourage you to try out custom inputs as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 08:47:04,392 [INFO] root: Registry: ['nvcr.io']\n",
      "2023-03-07 08:47:04,524 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2023-03-07 08:47:04,534 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2023-03-07 08:47:09 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:09 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:09 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:09 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2023-03-07 08:47:09 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2023-03-07 08:47:10 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2023-03-07 08:47:10 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:47:10 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:10 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:15 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:15 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:15 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:15 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2023-03-07 08:47:16 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2023-03-07 08:47:16 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:47:16 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:47:16 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:16 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:17 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/nlp/text_classification/scripts/infer_onnx.py:150: UserWarning: \n",
      "    'infer_onnx.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "[NeMo I 2023-03-07 08:47:17 tlt_logging:20] Experiment configuration:\n",
      "    restore_from: /results/text_classification/export/exported-model.eonnx\n",
      "    input_batch:\n",
      "    - by the end of no such thing the audience , like beatrice , has a watchful affection\n",
      "      for the monster .\n",
      "    - director rob marshall went out gunning to make a great one .\n",
      "    - uneasy mishmash of styles and genres .\n",
      "    - I love exotic science fiction / fantasy movies but this one was very unpleasant\n",
      "      to watch . Suggestions and images of child abuse , mutilated bodies (live or dead)\n",
      "      , other gruesome scenes , plot holes , boring acting made this a regretable experience\n",
      "      , The basic idea of entering another person's mind is not even new to the movies\n",
      "      or TV (An Outer Limits episode was better at exploring this idea) . i gave it 4\n",
      "      / 10 since some special effects were nice .\n",
      "    dataloader:\n",
      "      batch_size: 10\n",
      "      shuffle: false\n",
      "      sampler: null\n",
      "      batch_sampler: null\n",
      "      num_workers: 3\n",
      "      collate_fn: null\n",
      "      pin_memory: false\n",
      "      drop_last: false\n",
      "      timeout: 0\n",
      "      worker_init_fn: null\n",
      "      multiprocessing_context: null\n",
      "    encryption_key: '****'\n",
      "    exp_manager:\n",
      "      task_name: infer_onnx\n",
      "      explicit_log_dir: /results/text_classification/infer_onnx\n",
      "    \n",
      "[NeMo I 2023-03-07 08:47:26 tokenizer_utils:100] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /root/.cache/huggingface/nemo_nlp_tmp/ae0d012864bdb2474ba67537c3f5e0fa/tokenizer.vocab_file, special_tokens_dict: {}, and use_fast: False\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2023-03-07 08:47:26 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2023-03-07 08:47:26 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2023-03-07 08:47:33 infer_onnx:139] The prediction results of some sample queries with the trained model:\n",
      "[NeMo I 2023-03-07 08:47:33 infer_onnx:141] Query: by the end of no such thing the audience , like beatrice , has a watchful affection for the monster .\n",
      "[NeMo I 2023-03-07 08:47:33 infer_onnx:142] Predicted label: positive\n",
      "[NeMo I 2023-03-07 08:47:33 infer_onnx:141] Query: director rob marshall went out gunning to make a great one .\n",
      "[NeMo I 2023-03-07 08:47:33 infer_onnx:142] Predicted label: positive\n",
      "[NeMo I 2023-03-07 08:47:33 infer_onnx:141] Query: uneasy mishmash of styles and genres .\n",
      "[NeMo I 2023-03-07 08:47:33 infer_onnx:142] Predicted label: positive\n",
      "[NeMo I 2023-03-07 08:47:33 infer_onnx:141] Query: I love exotic science fiction / fantasy movies but this one was very unpleasant to watch . Suggestions and images of child abuse , mutilated bodies (live or dead) , other gruesome scenes , plot holes , boring acting made this a regretable experience , The basic idea of entering another person's mind is not even new to the movies or TV (An Outer Limits episode was better at exploring this idea) . i gave it 4 / 10 since some special effects were nice .\n",
      "[NeMo I 2023-03-07 08:47:33 infer_onnx:142] Predicted label: positive\n",
      "[NeMo I 2023-03-07 08:47:33 infer_onnx:146] Experiment logs saved to '/results/text_classification/infer_onnx'\n",
      "\u001b[0m\u001b[0m2023-03-07 08:47:35,406 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# 7. For inference using ONNX:\n",
    "!tao text_classification infer_onnx \\\n",
    "    -e $SPECS_DIR/infer_onnx.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $RESULTS_DIR/export/exported-model.eonnx \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/infer_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='export-riva'></a>\n",
    "### Export to Riva\n",
    "\n",
    "With TAO, you can also export your model in a format that can deployed using [NVIDIA Riva](https://developer.nvidia.com/riva), a highly performant application framework for multi-modal conversational AI services using GPUs! The same command for exporting to ONNX can be used here. The only small variation is the configuration for `export_format` in the spec file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 08:47:36,381 [INFO] root: Registry: ['nvcr.io']\n",
      "2023-03-07 08:47:36,512 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
      "2023-03-07 08:47:36,522 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/root/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "[NeMo W 2023-03-07 08:47:41 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:41 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:41 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:41 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2023-03-07 08:47:41 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2023-03-07 08:47:42 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[NeMo W 2023-03-07 08:47:42 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:47:42 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:42 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:47 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.decoder_dataset.TextNormalizationDecoderDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:47 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.tagger_dataset.TextNormalizationTaggerDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:47 experimental:27] Module <class 'nemo.collections.nlp.data.text_normalization.test_dataset.TextNormalizationTestDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:47 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n",
      "[NeMo W 2023-03-07 08:47:47 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchaudio-0.7.0a0+42d447d-py3.8-linux-x86_64.egg/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2023-03-07 08:47:48 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali._AudioTextDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:47:48 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder.DuplexDecoderModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2023-03-07 08:47:48 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tagger.DuplexTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:48 experimental:27] Module <class 'nemo.collections.nlp.models.duplex_text_normalization.duplex_tn.DuplexTextNormalizationModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-03-07 08:47:49 nemo_logging:349] /home/jenkins/agent/workspace/tlt-pytorch-main-nightly/nlp/text_classification/scripts/export.py:97: UserWarning: \n",
      "    'export.yaml' is validated against ConfigStore schema with the same name.\n",
      "    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "    \n",
      "[NeMo I 2023-03-07 08:47:49 tlt_logging:20] Experiment configuration:\n",
      "    restore_from: /results/text_classification/train/checkpoints/trained-model.tlt\n",
      "    export_to: tc-model.riva\n",
      "    export_format: RIVA\n",
      "    exp_manager:\n",
      "      task_name: export\n",
      "      explicit_log_dir: /results/text_classification/export_riva\n",
      "    encryption_key: '********'\n",
      "    \n",
      "[NeMo I 2023-03-07 08:47:51 tokenizer_utils:100] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /root/.cache/huggingface/nemo_nlp_tmp/ae0d012864bdb2474ba67537c3f5e0fa/tokenizer.vocab_file, special_tokens_dict: {}, and use_fast: False\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2023-03-07 08:47:52 modelPT:1062] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2023-03-07 08:47:52 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2023-03-07 08:48:02 export:56] Model restored from '/results/text_classification/train/checkpoints/trained-model.tlt'\n",
      "[NeMo I 2023-03-07 08:48:22 export:79] Experiment logs saved to '/results/text_classification/export_riva'\n",
      "[NeMo I 2023-03-07 08:48:22 export:80] Exported model to '/results/text_classification/export_riva/tc-model.riva'\n",
      "[NeMo I 2023-03-07 08:48:24 export:91] Exported model is compliant with Riva\n",
      "\u001b[0m\u001b[0m2023-03-07 08:48:25,949 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# 8. For export to Riva:\n",
    "!tao text_classification export \\\n",
    "    -e $SPECS_DIR/export.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/export_riva \\\n",
    "    export_format=RIVA \\\n",
    "    export_to=tc-model.riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is exported as `tc-model.riva` which is in a format suited for deployment in Riva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next?\n",
    "\n",
    "You can use TAO to build custom models for your own applications, or deploy the custom models to NVIDIA Riva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
